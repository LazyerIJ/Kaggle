{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "import PIL\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications import Xception, VGG16, ResNet50, InceptionV3\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras import layers, models, optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.applications.xception import Xception, preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "\n",
    "from numpy.random import seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2\n",
    "seed(SEED)\n",
    "tf.set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../input'\n",
    "TRAIN_IMG_PATH = os.path.join(DATA_PATH, 'train')\n",
    "TEST_IMG_PATH = os.path.join(DATA_PATH, 'test')\n",
    "TRAIN_CROP_PATH = os.path.join(DATA_PATH, 'train_crop_299')\n",
    "TEST_CROP_PATH = os.path.join(DATA_PATH, 'test_crop_299')\n",
    "\n",
    "df_train = pd.read_csv(os.path.join(DATA_PATH, 'train.csv'))\n",
    "df_test = pd.read_csv(os.path.join(DATA_PATH, 'test.csv'))\n",
    "df_class = pd.read_csv(os.path.join(DATA_PATH, 'class.csv'))\n",
    "image_size=299"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/yu4u/cutout-random-erasing\n",
    "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
    "    def eraser(input_img):\n",
    "        input_img = preprocess_input(input_img)\n",
    "        img_h, img_w, img_c = input_img.shape\n",
    "        p_1 = np.random.rand()\n",
    "\n",
    "        if p_1 > p:\n",
    "            return input_img\n",
    "\n",
    "        while True:\n",
    "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
    "            r = np.random.uniform(r_1, r_2)\n",
    "            w = int(np.sqrt(s / r))\n",
    "            h = int(np.sqrt(s * r))\n",
    "            left = np.random.randint(0, img_w)\n",
    "            top = np.random.randint(0, img_h)\n",
    "\n",
    "            if left + w <= img_w and top + h <= img_h:\n",
    "                break\n",
    "\n",
    "        if pixel_level:\n",
    "            c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
    "        else:\n",
    "            c = np.random.uniform(v_l, v_h)\n",
    "\n",
    "        input_img[top:top + h, left:left + w, :] = c\n",
    "\n",
    "        return input_img\n",
    "\n",
    "    return eraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MixupImageDataGenerator():\n",
    "    def __init__(self, generator, dataframe, directory, target, batch_size, img_size, alpha=0.2, subset=None, scale='rgb'):\n",
    "        \"\"\"Constructor for mixup image data generator.\n",
    "\n",
    "        Arguments:\n",
    "            generator {object} -- An instance of Keras ImageDataGenerator.\n",
    "            directory {str} -- Image directory.\n",
    "            batch_size {int} -- Batch size.\n",
    "            img_height {int} -- Image height in pixels.\n",
    "            img_width {int} -- Image width in pixels.\n",
    "\n",
    "        Keyword Arguments:\n",
    "            alpha {float} -- Mixup beta distribution alpha parameter. (default: {0.2})\n",
    "            subset {str} -- 'training' or 'validation' if validation_split is specified in\n",
    "            `generator` (ImageDataGenerator).(default: {None})\n",
    "        \"\"\"\n",
    "\n",
    "        self.batch_index = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = alpha\n",
    "        # First iterator yielding tuples of (x, y)\n",
    "        self.generator1 = generator.flow_from_dataframe(dataframe=dataframe, \n",
    "                                                        directory=directory,\n",
    "                                                        x_col = 'img_file',\n",
    "                                                        y_col = target,\n",
    "                                                        target_size=(image_size, image_size),\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        class_mode='categorical',\n",
    "                                                        seed=3,\n",
    "                                                        color_mode=scale,\n",
    "                                                        shuffle=True,)\n",
    "\n",
    "        # Second iterator yielding tuples of (x, y)\n",
    "        self.generator2 = generator.flow_from_dataframe(dataframe=dataframe, \n",
    "                                                        directory=directory,\n",
    "                                                        x_col = 'img_file',\n",
    "                                                        y_col = target,\n",
    "                                                        target_size=(image_size, image_size),\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        class_mode='categorical',\n",
    "                                                        seed=5,\n",
    "                                                        color_mode=scale,\n",
    "                                                        shuffle=True)\n",
    "\n",
    "\n",
    "        # Number of images across all classes in image directory.\n",
    "        self.n = self.generator1.samples\n",
    "\n",
    "    def reset_index(self):\n",
    "        \"\"\"Reset the generator indexes array.\n",
    "        \"\"\"\n",
    "\n",
    "        self.generator1._set_index_array()\n",
    "        self.generator2._set_index_array()\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.reset_index()\n",
    "\n",
    "    def reset(self):\n",
    "        self.batch_index = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        # round up\n",
    "        return (self.n + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "    def get_steps_per_epoch(self):\n",
    "        \"\"\"Get number of steps per epoch based on batch size and\n",
    "        number of images.\n",
    "\n",
    "        Returns:\n",
    "            int -- steps per epoch.\n",
    "        \"\"\"\n",
    "        if (num_samples % batch_size) > 0:\n",
    "            return (num_samples // batch_size) + 1\n",
    "        else:\n",
    "            return num_samples // batch_size\n",
    "\n",
    "    def __next__(self):\n",
    "        \"\"\"Get next batch input/output pair.\n",
    "\n",
    "        Returns:\n",
    "            tuple -- batch of input/output pair, (inputs, outputs).\n",
    "        \"\"\"\n",
    "\n",
    "        if self.batch_index == 0:\n",
    "            self.reset_index()\n",
    "\n",
    "        current_index = (self.batch_index * self.batch_size) % self.n\n",
    "        if self.n > current_index + self.batch_size:\n",
    "            self.batch_index += 1\n",
    "        else:\n",
    "            self.batch_index = 0\n",
    "        \n",
    "        # Get a pair of inputs and outputs from two iterators.\n",
    "        X1, y1 = self.generator1.next()\n",
    "        X2, y2 = self.generator2.next()\n",
    "        size = X1.shape[0]\n",
    "        # random sample the lambda value from beta distribution.\n",
    "        l = np.random.randint(90, 100, size=size) / 100\n",
    "        X_l = l.reshape(size, 1, 1, 1)\n",
    "        y_l = l.reshape(size, 1)\n",
    "\n",
    "        # Perform the mixup.\n",
    "        X = X1 * X_l + X2 * (1 - X_l)\n",
    "        y = y1 * y_l + y2 * (1 - y_l)\n",
    "        return X, y\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            yield next(self)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generator(train_df, val_df, train_dir, valid_dir, test_df, test_dir, image_size, batch_size,valid_batch_size,\n",
    "                 scale='rgb', target='class'):\n",
    "    train_generator = train_datagen.flow_from_dataframe(\n",
    "        dataframe=train_df, \n",
    "        directory=train_dir,\n",
    "        x_col = 'img_file',\n",
    "        y_col = target,\n",
    "        target_size=(image_size, image_size),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        seed=3,\n",
    "        color_mode=scale,\n",
    "        shuffle=True,\n",
    "        #preprocessing_function=get_random_eraser(v_l=0, v_h=1, pixel_level=True)\n",
    "    )\n",
    "\n",
    "    validation_generator = valid_datagen.flow_from_dataframe(\n",
    "        dataframe=val_df,\n",
    "        directory=valid_dir,\n",
    "        x_col = 'img_file',\n",
    "        y_col = target,\n",
    "        target_size=(image_size,image_size),\n",
    "        batch_size=valid_batch_size,\n",
    "        class_mode='categorical',\n",
    "        seed=3,\n",
    "        color_mode=scale\n",
    "        #processing_function=preprocess_input\n",
    "    )\n",
    "    test_generator = test_datagen.flow_from_dataframe(\n",
    "        dataframe=test_df,\n",
    "        directory=test_dir,\n",
    "        x_col='img_file',\n",
    "        y_col=None,\n",
    "        target_size= (image_size,image_size),\n",
    "        color_mode=scale,\n",
    "        class_mode=None,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        #preprocessing_function=preprocess_input\n",
    "    )\n",
    "    return train_generator, validation_generator, test_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False,\n",
    "    zoom_range=0.1\n",
    "    )\n",
    "valid_datagen = ImageDataGenerator()\n",
    "test_datagen = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"class\"] = df_train[\"class\"].astype('str')\n",
    "df_train = df_train[['img_file', 'class']]\n",
    "df_test = df_test[['img_file']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_traindf(df, train_size=0.6, stratify=True, label='class'):\n",
    "    target = None\n",
    "    if stratify:\n",
    "        target = df[label].values\n",
    "    X_train, X_val = train_test_split(df, train_size=train_size, random_state=SEED, stratify=target)\n",
    "    X_train = X_train.reset_index(drop=True)\n",
    "    X_val = X_val.reset_index(drop=True)\n",
    "    return X_train, X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val = split_traindf(df_train.iloc[:, :], train_size=0.8, stratify=True)\n",
    "nb_train_sample = X_train.shape[0]\n",
    "nb_validation_sample = X_val.shape[0]\n",
    "nb_test_sample = df_test.shape[0]\n",
    "scale = 'rgb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=16\n",
    "valid_batch_size=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8012 validated image filenames belonging to 196 classes.\n",
      "Found 2004 validated image filenames belonging to 196 classes.\n",
      "Found 6169 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "train_gen, validation_gen, test_gen = get_generator(train_df=X_train,\n",
    "                                                    val_df=X_val,\n",
    "                                                    train_dir=TRAIN_CROP_PATH,\n",
    "                                                    valid_dir=TRAIN_CROP_PATH,\n",
    "                                                    test_df=df_test,\n",
    "                                                    test_dir=TEST_CROP_PATH,\n",
    "                                                    image_size=image_size,\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    scale=scale,\n",
    "                                                    valid_batch_size=valid_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8012 validated image filenames belonging to 196 classes.\n",
      "Found 8012 validated image filenames belonging to 196 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen = MixupImageDataGenerator(generator = train_datagen,\n",
    "                                    dataframe = X_train,\n",
    "                                    directory=TRAIN_CROP_PATH,\n",
    "                                    batch_size=batch_size,\n",
    "                                    img_size=image_size,\n",
    "                                    target='class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(app, image_size, opt, num_class=196, lr=0.0001):\n",
    "    if app=='Xception':\n",
    "        application = Xception\n",
    "    elif app=='VGG16':\n",
    "        application = VGG16\n",
    "    elif app=='ResNet50':\n",
    "        application = ResNet50\n",
    "    elif app=='InceptionV3':\n",
    "        application = InceptionV3\n",
    "    base_model = application(weights='imagenet', input_shape=(image_size,image_size,3), include_top=False)\n",
    "    #base_model.trainable = False\n",
    "    \n",
    "\n",
    "    model = models.Sequential()\n",
    "    model.add(base_model)\n",
    "    model.add(layers.GlobalAveragePooling2D())\n",
    "    model.add(layers.Dense(512, activation='sigmoid'))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Dense(num_class, activation='softmax'))\n",
    "    #model.summary()\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['acc'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_path(model_dir, model_name, model_type):\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.mkdir(model_dir)\n",
    "    from datetime import datetime\n",
    "    now = datetime.now()\n",
    "    date_time = now.strftime(\"%m%d_%H%M\")\n",
    "    model_path =model_dir +  \"{}_{}_{}.{}\".format(model_name , date_time ,model_type, '.hdf5')\n",
    "    print('>>model path to save: {}'.format(model_path))\n",
    "    return model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type='Xception'\n",
    "image_size = 299 if model_type=='Xception' else 224\n",
    "histories=[]\n",
    "patient = 10\n",
    "lr = 0.0001\n",
    "epoch=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>model path to save: ../model/baseline_mixup_0708_0704_Xception..hdf5\n",
      ">>get model completed\n"
     ]
    }
   ],
   "source": [
    "model_name = 'baseline_mixup'\n",
    "model_dir = '../model/'\n",
    "model_path = get_model_path(model_dir, model_name, model_type)\n",
    "model = get_model(app=model_type, image_size=image_size, opt=optimizers.RMSprop(lr=lr), lr=lr)\n",
    "print('>>get model completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "\n",
    "class printLearningRate(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        lr = self.model.optimizer.lr\n",
    "        decay = self.model.optimizer.decay\n",
    "        iterations = self.model.optimizer.iterations\n",
    "        lr_with_decay = lr / (1. + decay * K.cast(iterations, K.dtype(decay)))\n",
    "        print('Current LR : ', K.eval(lr_with_decay))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callback(patient, model_path, lr, total_count):\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss',\n",
    "                      patience=patient,\n",
    "                      mode='min',\n",
    "                      verbose=1),\n",
    "        ModelCheckpoint(filepath=model_path,\n",
    "                        monitor='val_loss',\n",
    "                        verbose=1,\n",
    "                        save_best_only=True,\n",
    "                        mode='min'),\n",
    "        ReduceLROnPlateau(monitor = 'val_loss',\n",
    "                          patience = 1,\n",
    "                          min_lr=lr * 0.0001,\n",
    "                          verbose=True,\n",
    "                          mode='min'),\n",
    "        printLearningRate()\n",
    "    ]\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_steps(num_samples, batch_size):\n",
    "    if (num_samples % batch_size) > 0:\n",
    "        return (num_samples // batch_size) + 1\n",
    "    else:\n",
    "        return num_samples // batch_size\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "501/501 [==============================] - 201s 401ms/step - loss: 3.3749 - acc: 0.3523 - val_loss: 2.3352 - val_acc: 0.5374\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.33516, saving model to ../model/baseline_mixup_0708_0704_Xception..hdf5\n",
      "Current LR :  1e-04\n",
      "Epoch 2/300\n",
      "501/501 [==============================] - 201s 401ms/step - loss: 2.3922 - acc: 0.5850 - val_loss: 1.5205 - val_acc: 0.7131\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.33516 to 1.52048, saving model to ../model/baseline_mixup_0708_0704_Xception..hdf5\n",
      "Current LR :  1e-04\n",
      "Epoch 3/300\n",
      "501/501 [==============================] - 201s 401ms/step - loss: 1.6856 - acc: 0.7359 - val_loss: 0.9667 - val_acc: 0.8034\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.52048 to 0.96674, saving model to ../model/baseline_mixup_0708_0704_Xception..hdf5\n",
      "Current LR :  1e-04\n",
      "Epoch 4/300\n",
      "501/501 [==============================] - 201s 401ms/step - loss: 1.2295 - acc: 0.8329 - val_loss: 0.6828 - val_acc: 0.8413\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.96674 to 0.68280, saving model to ../model/baseline_mixup_0708_0704_Xception..hdf5\n",
      "Current LR :  1e-04\n",
      "Epoch 5/300\n",
      "501/501 [==============================] - 200s 399ms/step - loss: 0.9730 - acc: 0.8899 - val_loss: 0.5564 - val_acc: 0.8618\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.68280 to 0.55638, saving model to ../model/baseline_mixup_0708_0704_Xception..hdf5\n",
      "Current LR :  1e-04\n",
      "Epoch 6/300\n",
      "501/501 [==============================] - 201s 402ms/step - loss: 0.8373 - acc: 0.9214 - val_loss: 0.5223 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.55638 to 0.52232, saving model to ../model/baseline_mixup_0708_0704_Xception..hdf5\n",
      "Current LR :  1e-04\n",
      "Epoch 7/300\n",
      "501/501 [==============================] - 200s 400ms/step - loss: 0.7597 - acc: 0.9440 - val_loss: 0.4361 - val_acc: 0.8882\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.52232 to 0.43608, saving model to ../model/baseline_mixup_0708_0704_Xception..hdf5\n",
      "Current LR :  1e-04\n",
      "Epoch 8/300\n",
      "501/501 [==============================] - 201s 400ms/step - loss: 0.7148 - acc: 0.9598 - val_loss: 0.4375 - val_acc: 0.8967\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.43608\n",
      "Current LR :  1e-04\n",
      "Epoch 9/300\n",
      "501/501 [==============================] - 202s 402ms/step - loss: 0.6837 - acc: 0.9657 - val_loss: 0.4649 - val_acc: 0.8822\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.43608\n",
      "Current LR :  1e-04\n",
      "Epoch 10/300\n",
      "501/501 [==============================] - 201s 402ms/step - loss: 0.6604 - acc: 0.9737 - val_loss: 0.3967 - val_acc: 0.9037\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.43608 to 0.39665, saving model to ../model/baseline_mixup_0708_0704_Xception..hdf5\n",
      "Current LR :  1e-04\n",
      "Epoch 11/300\n",
      "501/501 [==============================] - 201s 402ms/step - loss: 0.6384 - acc: 0.9799 - val_loss: 0.4274 - val_acc: 0.9017\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.39665\n",
      "Current LR :  1e-04\n",
      "Epoch 12/300\n",
      "501/501 [==============================] - 201s 402ms/step - loss: 0.6305 - acc: 0.9829 - val_loss: 0.4245 - val_acc: 0.9022\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.39665\n",
      "Current LR :  1e-04\n",
      "Epoch 13/300\n",
      "501/501 [==============================] - 200s 400ms/step - loss: 0.6192 - acc: 0.9852 - val_loss: 0.4358 - val_acc: 0.9022\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.39665\n",
      "Current LR :  1e-04\n",
      "Epoch 14/300\n",
      "501/501 [==============================] - 200s 399ms/step - loss: 0.6064 - acc: 0.9869 - val_loss: 0.4255 - val_acc: 0.9097\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.39665\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "Current LR :  1e-05\n",
      "Epoch 15/300\n",
      "501/501 [==============================] - 200s 399ms/step - loss: 0.5800 - acc: 0.9928 - val_loss: 0.3714 - val_acc: 0.9162\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.39665 to 0.37144, saving model to ../model/baseline_mixup_0708_0704_Xception..hdf5\n",
      "Current LR :  1e-05\n",
      "Epoch 16/300\n",
      "501/501 [==============================] - 200s 399ms/step - loss: 0.5683 - acc: 0.9951 - val_loss: 0.3676 - val_acc: 0.9227\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.37144 to 0.36761, saving model to ../model/baseline_mixup_0708_0704_Xception..hdf5\n",
      "Current LR :  1e-05\n",
      "Epoch 17/300\n",
      "501/501 [==============================] - 200s 398ms/step - loss: 0.5645 - acc: 0.9955 - val_loss: 0.3727 - val_acc: 0.9187\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.36761\n",
      "Current LR :  1e-05\n",
      "Epoch 18/300\n",
      "501/501 [==============================] - 199s 398ms/step - loss: 0.5587 - acc: 0.9951 - val_loss: 0.3785 - val_acc: 0.9192\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.36761\n",
      "Current LR :  1e-05\n",
      "Epoch 19/300\n",
      "501/501 [==============================] - 199s 398ms/step - loss: 0.5548 - acc: 0.9969 - val_loss: 0.3773 - val_acc: 0.9182\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.36761\n",
      "Current LR :  1e-05\n",
      "Epoch 20/300\n",
      "501/501 [==============================] - 200s 398ms/step - loss: 0.5601 - acc: 0.9960 - val_loss: 0.3698 - val_acc: 0.9251\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.36761\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "Current LR :  1e-06\n",
      "Epoch 21/300\n",
      "501/501 [==============================] - 199s 398ms/step - loss: 0.5582 - acc: 0.9961 - val_loss: 0.3710 - val_acc: 0.9217\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.36761\n",
      "Current LR :  1e-06\n",
      "Epoch 22/300\n",
      "214/501 [===========>..................] - ETA: 1:48 - loss: 0.5505 - acc: 0.9968"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-04155f94160b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_steps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_validation_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m )\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "    train_gen,\n",
    "    steps_per_epoch=get_steps(nb_train_sample, batch_size),\n",
    "    epochs=epoch,\n",
    "    validation_data=validation_gen,\n",
    "    validation_steps=get_steps(nb_validation_sample, valid_batch_size),\n",
    "    verbose=1,\n",
    "    callbacks=get_callback(patient, model_path, lr, len(X_train))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use 5000 images\n",
    "baseline + ratio image test\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
