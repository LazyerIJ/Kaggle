{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "import PIL\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.resnet_v2 import ResNet152V2\n",
    "from keras.applications.mobilenet_v2 import MobileNetV2\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras import layers, models, optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/qubvel/efficientnet\n",
    "from efficientnet import EfficientNetB3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model', '.ipynb_checkpoints', 'input', 'src', 'kernels']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['class.csv',\n",
       " 'train_crop_299',\n",
       " 'sample_submission.csv',\n",
       " '.ipynb_checkpoints',\n",
       " 'train.csv',\n",
       " 'test.csv',\n",
       " 'test_crop_299']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('../input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_file</th>\n",
       "      <th>bbox_x1</th>\n",
       "      <th>bbox_y1</th>\n",
       "      <th>bbox_x2</th>\n",
       "      <th>bbox_y2</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_00001.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "      <td>641</td>\n",
       "      <td>461</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_00002.jpg</td>\n",
       "      <td>57</td>\n",
       "      <td>53</td>\n",
       "      <td>293</td>\n",
       "      <td>236</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_00003.jpg</td>\n",
       "      <td>35</td>\n",
       "      <td>42</td>\n",
       "      <td>227</td>\n",
       "      <td>160</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          img_file  bbox_x1  bbox_y1  bbox_x2  bbox_y2  class\n",
       "0  train_00001.jpg        1       80      641      461    108\n",
       "1  train_00002.jpg       57       53      293      236     71\n",
       "2  train_00003.jpg       35       42      227      160     76"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('../input/train.csv').head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2\n",
    "seed(SEED)\n",
    "tf.set_random_seed(SEED)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image data normalization\n",
    "def basic_preprocess_input(x):\n",
    "    x /= 255.\n",
    "    x -= 0.5\n",
    "    x *= 2.\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/yu4u/cutout-random-erasing\n",
    "#image augmentation - randombox\n",
    "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.25, r_1=0.3,\n",
    "                      r_2=1/0.3, v_l=0, v_h=255, pixel_level=True, ismixup=False):\n",
    "    def eraser(input_img, ismixup=ismixup):\n",
    "        if not ismixup:\n",
    "            input_img = basic_preprocess_input(input_img)\n",
    "        img_h, img_w, img_c = input_img.shape\n",
    "        p_1 = np.random.rand()\n",
    "        \n",
    "        if p_1 > p:\n",
    "            return input_img\n",
    "\n",
    "        while True:\n",
    "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
    "            r = np.random.uniform(r_1, r_2)\n",
    "            w = int(np.sqrt(s / r))\n",
    "            h = int(np.sqrt(s * r))\n",
    "            left = np.random.randint(0, img_w)\n",
    "            top = np.random.randint(0, img_h)\n",
    "\n",
    "            if left + w*1.3 <= img_w and top + h*1.2 <= img_h:\n",
    "                break\n",
    "\n",
    "        if pixel_level:\n",
    "            c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
    "        else:\n",
    "            c = np.random.uniform(v_l, v_h)\n",
    "\n",
    "        input_img[top:top + h, left:left + w, :] = c\n",
    "\n",
    "        return input_img\n",
    "\n",
    "    return eraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image augmentation - mixup\n",
    "class MixupImageDataGenerator():\n",
    "    def __init__(self, generator, dataframe, directory, target, batch_size, image_size, israndombox,\n",
    "                 alpha=0.2, subset=None, scale='rgb',min_mixup=95, max_mixup=100, x_col='img_file'):\n",
    "        \"\"\"Constructor for mixup image data generator.\n",
    "\n",
    "        Arguments:\n",
    "            generator {object} -- An instance of Keras ImageDataGenerator.\n",
    "            directory {str} -- Image directory.\n",
    "            batch_size {int} -- Batch size.\n",
    "            img_height {int} -- Image height in pixels.\n",
    "            img_width {int} -- Image width in pixels.\n",
    "\n",
    "        Keyword Arguments:\n",
    "            alpha {float} -- Mixup beta distribution alpha parameter. (default: {0.2})\n",
    "            subset {str} -- 'training' or 'validation' if validation_split is specified in\n",
    "            `generator` (ImageDataGenerator).(default: {None})\n",
    "        \"\"\"\n",
    "\n",
    "        self.batch_index = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = alpha\n",
    "        self.min_mixup=min_mixup\n",
    "        self.max_mixup=max_mixup\n",
    "        self.israndombox = israndombox\n",
    "        # First iterator yielding tuples of (x, y)\n",
    "        self.erasor = get_random_eraser(ismixup=True)\n",
    "        \n",
    "        self.generator1 = generator.flow_from_dataframe(dataframe=dataframe, \n",
    "                                                        directory=directory,\n",
    "                                                        x_col = x_col,\n",
    "                                                        y_col = target,\n",
    "                                                        target_size=(image_size, image_size),\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        class_mode='categorical',\n",
    "                                                        seed=3,\n",
    "                                                        color_mode=scale,\n",
    "                                                        shuffle=True)\n",
    "\n",
    "        # Second iterator yielding tuples of (x, y)\n",
    "        self.generator2 = generator.flow_from_dataframe(dataframe=dataframe, \n",
    "                                                        directory=directory,\n",
    "                                                        x_col = x_col,\n",
    "                                                        y_col = target,\n",
    "                                                        target_size=(image_size, image_size),\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        class_mode='categorical',\n",
    "                                                        seed=np.random.randint(100),\n",
    "                                                        color_mode=scale,\n",
    "                                                        shuffle=True)\n",
    "\n",
    "\n",
    "        # Number of images across all classes in image directory.\n",
    "        self.n = self.generator1.samples\n",
    "\n",
    "    def reset_index(self):\n",
    "        \"\"\"Reset the generator indexes array.\n",
    "        \"\"\"\n",
    "\n",
    "        self.generator1._set_index_array()\n",
    "        self.generator2._set_index_array()\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.reset_index()\n",
    "\n",
    "    def reset(self):\n",
    "        self.batch_index = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        # round up\n",
    "        return (self.n + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "    def get_steps_per_epoch(self):\n",
    "        \"\"\"Get number of steps per epoch based on batch size and\n",
    "        number of images.\n",
    "\n",
    "        Returns:\n",
    "            int -- steps per epoch.\n",
    "        \"\"\"\n",
    "        if (num_samples % batch_size) > 0:\n",
    "            return (num_samples // batch_size) + 1\n",
    "        else:\n",
    "            return num_samples // batch_size\n",
    "\n",
    "    def __next__(self):\n",
    "        \"\"\"Get next batch input/output pair.\n",
    "\n",
    "        Returns:\n",
    "            tuple -- batch of input/output pair, (inputs, outputs).\n",
    "        \"\"\"\n",
    "\n",
    "        if self.batch_index == 0:\n",
    "            self.reset_index()\n",
    "\n",
    "        current_index = (self.batch_index * self.batch_size) % self.n\n",
    "        if self.n > current_index + self.batch_size:\n",
    "            self.batch_index += 1\n",
    "        else:\n",
    "            self.batch_index = 0\n",
    "        \n",
    "        # Get a pair of inputs and outputs from two iterators.\n",
    "        X1, y1 = self.generator1.next()\n",
    "        X2, y2 = self.generator2.next()\n",
    "        size = X1.shape[0]\n",
    "        # random sample the lambda value from beta distribution.\n",
    "        l = np.random.randint(self.min_mixup,self. max_mixup, size=size) / 100\n",
    "        X_l = l.reshape(size, 1, 1, 1)\n",
    "        y_l = l.reshape(size, 1)\n",
    "\n",
    "        # Perform the mixup.\n",
    "        X = X1 * X_l + X2 * (1 - X_l)\n",
    "        if self.israndombox:\n",
    "            for idx in range(size):\n",
    "                X[idx] = self.erasor(X[idx]) \n",
    "        y = y1 * y_l + y2 * (1 - y_l)\n",
    "        \n",
    "        return X, y\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            yield next(self)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data generator\n",
    "def get_generator(train_df, val_df, test_df,\n",
    "                  train_datagen,\n",
    "                  valid_datagen,\n",
    "                  test_datagen,\n",
    "                  image_size,\n",
    "                  ismixup=True,\n",
    "                  israndombox=True,\n",
    "                  batch_size=16,\n",
    "                  valid_batch_size=16,\n",
    "                  scale='rgb',\n",
    "                  target='class',\n",
    "                  min_mixup=95,\n",
    "                  max_mixup=100,\n",
    "                  test_batch_size=32):\n",
    "    \n",
    "    train_crop_path = os.path.join(DATA_PATH, 'train_crop_{}'.format(image_size))\n",
    "    test_crop_path = os.path.join(DATA_PATH, 'test_crop_{}'.format(image_size))\n",
    "    if ismixup:\n",
    "        train_generator = MixupImageDataGenerator(generator = train_datagen,\n",
    "                                                    dataframe = train_df,\n",
    "                                                    directory=train_crop_path,\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    image_size=image_size,\n",
    "                                                    target='class',\n",
    "                                                    israndombox=israndombox,\n",
    "                                                    min_mixup=min_mixup,\n",
    "                                                    max_mixup=max_mixup)\n",
    "    else:\n",
    "        train_generator = train_datagen.flow_from_dataframe(\n",
    "            dataframe=train_df, \n",
    "            directory=train_crop_path,\n",
    "            x_col = 'img_file',\n",
    "            y_col = target,\n",
    "            target_size=(image_size, image_size),\n",
    "            batch_size=batch_size,\n",
    "            class_mode='categorical',\n",
    "            seed=3,\n",
    "            color_mode=scale,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "    validation_generator = valid_datagen.flow_from_dataframe(\n",
    "        dataframe=val_df,\n",
    "        directory=train_crop_path,\n",
    "        x_col = 'img_file',\n",
    "        y_col = target,\n",
    "        target_size=(image_size,image_size),\n",
    "        batch_size=valid_batch_size,\n",
    "        class_mode='categorical',\n",
    "        seed=3,\n",
    "        color_mode=scale,\n",
    "    )\n",
    "    test_generator = test_datagen.flow_from_dataframe(\n",
    "        dataframe=test_df,\n",
    "        directory=test_crop_path,\n",
    "        x_col='img_file',\n",
    "        y_col=None,\n",
    "        target_size= (image_size,image_size),\n",
    "        color_mode=scale,\n",
    "        class_mode=None,\n",
    "        batch_size=test_batch_size,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    return train_generator, validation_generator, test_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_path(model_dir, model_name, model_type, fold_step=1):\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.mkdir(model_dir)\n",
    "    from datetime import datetime\n",
    "    now = datetime.now()\n",
    "    date_time = now.strftime(\"%m%d%H%M\")\n",
    "    model_path =model_dir +  \"{}_{}_{}_fold_{}.{}\".format(model_name, model_type, date_time, fold_step, 'hdf5')\n",
    "    model_path =model_dir +  \"{}_{}_fold_{}.{}\".format(model_name, model_type, fold_step, 'hdf5')\n",
    "\n",
    "    print('>>model path to save: {}'.format(model_path))\n",
    "    return model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class printLearningRate(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        lr = self.model.optimizer.lr\n",
    "        decay = self.model.optimizer.decay\n",
    "        iterations = self.model.optimizer.iterations\n",
    "        lr_with_decay = lr / (1. + decay * K.cast(iterations, K.dtype(decay)))\n",
    "        print('Current LR : ', K.eval(lr_with_decay))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cosine_decay_with_warmup(global_step,\n",
    "                             learning_rate_base,\n",
    "                             total_steps,\n",
    "                             warmup_learning_rate=0.0,\n",
    "                             warmup_steps=0,\n",
    "                             hold_base_rate_steps=0):\n",
    "    \"\"\"Cosine decay schedule with warm up period.\n",
    "\n",
    "    Cosine annealing learning rate as described in:\n",
    "      Loshchilov and Hutter, SGDR: Stochastic Gradient Descent with Warm Restarts.\n",
    "      ICLR 2017. https://arxiv.org/abs/1608.03983\n",
    "    In this schedule, the learning rate grows linearly from warmup_learning_rate\n",
    "    to learning_rate_base for warmup_steps, then transitions to a cosine decay\n",
    "    schedule.\n",
    "\n",
    "    Arguments:\n",
    "        global_step {int} -- global step.\n",
    "        learning_rate_base {float} -- base learning rate.\n",
    "        total_steps {int} -- total number of training steps.\n",
    "\n",
    "    Keyword Arguments:\n",
    "        warmup_learning_rate {float} -- initial learning rate for warm up. (default: {0.0})\n",
    "        warmup_steps {int} -- number of warmup steps. (default: {0})\n",
    "        hold_base_rate_steps {int} -- Optional number of steps to hold base learning rate\n",
    "                                    before decaying. (default: {0})\n",
    "    Returns:\n",
    "      a float representing learning rate.\n",
    "\n",
    "    Raises:\n",
    "      ValueError: if warmup_learning_rate is larger than learning_rate_base,\n",
    "        or if warmup_steps is larger than total_steps.\n",
    "    \"\"\"\n",
    "\n",
    "    if total_steps < warmup_steps:\n",
    "        raise ValueError('total_steps must be larger or equal to '\n",
    "                         'warmup_steps.')\n",
    "    learning_rate = 0.5 * learning_rate_base * (1 + np.cos(\n",
    "        np.pi *\n",
    "        (global_step - warmup_steps - hold_base_rate_steps\n",
    "         ) / float(total_steps - warmup_steps - hold_base_rate_steps)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    if hold_base_rate_steps > 0:\n",
    "        learning_rate = np.where(global_step > warmup_steps + hold_base_rate_steps,\n",
    "                                 learning_rate, learning_rate_base)\n",
    "    if warmup_steps > 0:\n",
    "        if learning_rate_base < warmup_learning_rate:\n",
    "            raise ValueError('learning_rate_base must be larger or equal to '\n",
    "                             'warmup_learning_rate.')\n",
    "        slope = (learning_rate_base - warmup_learning_rate) / warmup_steps\n",
    "        warmup_rate = slope * global_step + warmup_learning_rate\n",
    "        learning_rate = np.where(global_step < warmup_steps, warmup_rate,\n",
    "                                 learning_rate)\n",
    "    return np.where(global_step > total_steps, 0.0, learning_rate)\n",
    "\n",
    "\n",
    "class WarmUpCosineDecayScheduler(keras.callbacks.Callback):\n",
    "    \"\"\"Cosine decay with warmup learning rate scheduler\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 learning_rate_base,\n",
    "                 total_steps,\n",
    "                 global_step_init=0,\n",
    "                 warmup_learning_rate=0.0,\n",
    "                 warmup_steps=0,\n",
    "                 hold_base_rate_steps=0,\n",
    "                 verbose=0):\n",
    "        \"\"\"Constructor for cosine decay with warmup learning rate scheduler.\n",
    "\n",
    "    Arguments:\n",
    "        learning_rate_base {float} -- base learning rate.\n",
    "        total_steps {int} -- total number of training steps.\n",
    "\n",
    "    Keyword Arguments:\n",
    "        global_step_init {int} -- initial global step, e.g. from previous checkpoint.\n",
    "        warmup_learning_rate {float} -- initial learning rate for warm up. (default: {0.0})\n",
    "        warmup_steps {int} -- number of warmup steps. (default: {0})\n",
    "        hold_base_rate_steps {int} -- Optional number of steps to hold base learning rate\n",
    "                                    before decaying. (default: {0})\n",
    "        verbose {int} -- 0: quiet, 1: update messages. (default: {0})\n",
    "        \"\"\"\n",
    "\n",
    "        super(WarmUpCosineDecayScheduler, self).__init__()\n",
    "        self.learning_rate_base = learning_rate_base\n",
    "        self.total_steps = total_steps\n",
    "        self.global_step = global_step_init\n",
    "        self.warmup_learning_rate = warmup_learning_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.hold_base_rate_steps = hold_base_rate_steps\n",
    "        self.verbose = verbose\n",
    "        self.learning_rates = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        self.global_step = self.global_step + 1\n",
    "        lr = K.get_value(self.model.optimizer.lr)\n",
    "        self.learning_rates.append(lr)\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        lr = cosine_decay_with_warmup(global_step=self.global_step,\n",
    "                                      learning_rate_base=self.learning_rate_base,\n",
    "                                      total_steps=self.total_steps,\n",
    "                                      warmup_learning_rate=self.warmup_learning_rate,\n",
    "                                      warmup_steps=self.warmup_steps,\n",
    "                                      hold_base_rate_steps=self.hold_base_rate_steps)\n",
    "        K.set_value(self.model.optimizer.lr, lr)\n",
    "        if self.verbose > 0:\n",
    "            print('\\nBatch %05d: setting learning '\n",
    "                  'rate to %s.' % (self.global_step + 1, lr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_warmup_lr(base_lr, total_count, warmup_epoch=3):\n",
    "    \n",
    "\n",
    "    total_steps = int(epoch * total_count / batch_size)\n",
    "    # Compute the number of warmup batches.\n",
    "    warmup_steps = int(warmup_epoch * total_count / batch_size)\n",
    "    # Compute the number of warmup batches.\n",
    "    warmup_batches = warmup_epoch * total_count / batch_size\n",
    "\n",
    "    # Create the Learning rate scheduler.\n",
    "    warm_up_lr = WarmUpCosineDecayScheduler(learning_rate_base=lr,\n",
    "                                            total_steps=total_steps,\n",
    "                                            warmup_learning_rate=lr*0.5,\n",
    "                                            warmup_steps=warmup_steps,\n",
    "                                            hold_base_rate_steps=0)\n",
    "    return warm_up_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_type, image_size, opt, num_class=196, lr=0.0001, activation='sigmoid',\n",
    "             dense=512, drop=0.2):\n",
    "    #'xception', 'vgg19', 'resnet50', 'inception_v3', 'resnet_v2', 'mobilenet_v2', 'inception_resnet_v2'\n",
    "    if model_type=='xception':\n",
    "        application = Xception\n",
    "    elif model_type=='vgg19':\n",
    "        application = VGG19\n",
    "    elif model_type=='resnet50':\n",
    "        application = ResNet50\n",
    "    elif model_type=='inception_v3':\n",
    "        application = InceptionV3\n",
    "    elif model_type=='resnet_v2':\n",
    "        application = ResNet152V2\n",
    "    elif model_type=='mobilenet_v2':\n",
    "        application = MobileNetV2\n",
    "    elif model_type=='inception_resnet_v2':\n",
    "        application = InceptionResNetV2\n",
    "    elif model_type=='EfficientNetB3':\n",
    "        application = EfficientNetB3\n",
    "    else:\n",
    "        application = 'Xception'\n",
    "        print('>>set application to Xception(Not selected)')\n",
    "    base_model = application(weights='imagenet', input_shape=(image_size,image_size,3), include_top=False)\n",
    "    #base_model.trainable = False\n",
    "    model = models.Sequential()\n",
    "    model.add(base_model)\n",
    "    model.add(layers.GlobalAveragePooling2D())\n",
    "    model.add(layers.Dense(dense, activation=activation, kernel_initializer='he_normal'))\n",
    "    model.add(layers.Dropout(drop))\n",
    "    model.add(layers.Dense(num_class, activation='softmax', kernel_initializer='lecun_normal'))\n",
    "    #model.summary()\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['acc', f1_m])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callback(patient, model_path, lr, total_count, cosine, min_lr=0.00001, warmup_epoch=10):\n",
    "    #learning rate scheduler\n",
    "    if cosine:\n",
    "        #with cosine decay\n",
    "        warmup = get_warmup_lr(lr, total_count)\n",
    "    else:\n",
    "        #with Palateau\n",
    "        warmup = ReduceLROnPlateau(monitor = 'val_loss',\n",
    "                          factor = 0.5, patience = patient / 4,\n",
    "                          min_lr=min_lr, verbose=1, mode='min',\n",
    "                                  warmup_epoch=warmup_epoch)\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_f1_m',\n",
    "                      patience=patient,\n",
    "                      mode='max',\n",
    "                      verbose=1),\n",
    "        ModelCheckpoint(filepath=model_path,\n",
    "                        monitor='val_f1_m',\n",
    "                        verbose=1,\n",
    "                        save_best_only=True,\n",
    "                        mode='max'),\n",
    "        \n",
    "        warmup\n",
    "    ]\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_steps(num_samples, batch_size):\n",
    "    if (num_samples % batch_size) > 0:\n",
    "        return (num_samples // batch_size) + 1\n",
    "    else:\n",
    "        return num_samples // batch_size\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get generator with augmentation\n",
    "def get_datagen(ismixup, israndombox):\n",
    "    if israndombox and not ismixup:\n",
    "        train_datagen = ImageDataGenerator(\n",
    "            rotation_range=20,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            horizontal_flip=True,\n",
    "            vertical_flip=False,\n",
    "            zoom_range=0.2,\n",
    "            #shear_range=0.5,\n",
    "            #brightness_range=[0.5, 1.5],\n",
    "            preprocessing_function=get_random_eraser(v_l=0, v_h=1))  \n",
    "    \n",
    "    else:\n",
    "        train_datagen = ImageDataGenerator(\n",
    "            rotation_range=20,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            horizontal_flip=True,\n",
    "            vertical_flip=False,\n",
    "            zoom_range=0.2,\n",
    "            #shear_range=0.5,\n",
    "            #brightness_range=[0.5, 1.5],\n",
    "            preprocessing_function=basic_preprocess_input\n",
    "        )\n",
    "    \n",
    "    valid_datagen = ImageDataGenerator(\n",
    "        preprocessing_function=basic_preprocess_input)\n",
    "    \n",
    "    test_datagen = ImageDataGenerator(\n",
    "        preprocessing_function=basic_preprocess_input)\n",
    "    \n",
    "    return train_datagen, valid_datagen, test_datagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, model_name):\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title(model_name)\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.ylim(0.0, 1.0)\n",
    "    plt.grid(color='b', linestyle='-', linewidth=0.1, which='both')\n",
    "    plt.yticks([x/100 for x in range(0, 100,5)])\n",
    "    plt.xticks([x for x in range(0, 60,5)])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_size(model_type):\n",
    "    if model_type in ['xception', 'inception_v3', 'inception_resnet_v2','EfficientNetB3']:\n",
    "        return 299\n",
    "    elif model_type in ['vgg19', 'resnet50', 'resnet_v2', 'mobilenet_v2']:\n",
    "        return 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'xception':{'activation':'sigmoid',\n",
    "                     'dense':512,\n",
    "                     'drop':0.25\n",
    "                     },\n",
    "          'vgg19':{'activation':'relu',\n",
    "                                'dense':2048,\n",
    "                                'drop':0.25},\n",
    "          'resnet50':{'activation':'relu',\n",
    "                                'dense':2048,\n",
    "                                'drop':0.25},\n",
    "          'inception_v3':{'activation':'relu',\n",
    "                                'dense':2048,\n",
    "                                'drop':0.25},\n",
    "          'inception_resnet_v2':{'activation':'relu',\n",
    "                                'dense':1024,\n",
    "                                'drop':0.5},\n",
    "          'resnet_v2':{'activation':'relu',\n",
    "                                'dense':2048,\n",
    "                                'drop':0.25},\n",
    "          'mobilenet_v2':{'activation':'relu',\n",
    "                                'dense':2048,\n",
    "                                'drop':0.25},\n",
    "          'EfficientNetB3':{'activation':'relu',\n",
    "                                'dense':2048,\n",
    "                                'drop':0.25}\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get optimizer\n",
    "def get_opt(opt, lr=0.0001):\n",
    "    if opt=='sgd':\n",
    "        return optimizers.SGD(lr=lr, decay=1e-6, momentum=0.2, nesterov=True)\n",
    "    elif opt=='nadam':\n",
    "        return optimizers.Nadam(lr=lr)\n",
    "    elif opt=='rmsprop':\n",
    "        return optimizers.RMSprop(lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_type, skf_seed, opt, n_splits=4):\n",
    "    rs_models = {}\n",
    "    rs_histories = {}\n",
    "    \n",
    "    K.clear_session()\n",
    "    print('[*]-------------------Start train {} model-------------------'.format(model_type))\n",
    "    #data split -> kfold\n",
    "    skf = StratifiedKFold(n_splits=n_splits, random_state=skf_seed)\n",
    "    \n",
    "    for fold_step, (train_index, valid_index) in enumerate(skf.split(df_train['img_file'], df_train['class'])):\n",
    "        print('[*]-------------------{} model_foldstep_{}-------------------'.format(model_type, fold_step))\n",
    "        #get image size\n",
    "        image_size = get_image_size(model_type)\n",
    "        print('>>image_size: {}'.format(image_size))\n",
    "        #split train / validation set\n",
    "        X_train = df_train.iloc[train_index, :].reset_index()\n",
    "        X_val = df_train.iloc[valid_index, :].reset_index()\n",
    "        #get data generator\n",
    "        train_gen, valid_gen, test_gen = get_generator(train_df=X_train,\n",
    "                                                        val_df=X_val,\n",
    "                                                        test_df=df_test,\n",
    "                                                        train_datagen = train_datagen,\n",
    "                                                        valid_datagen = valid_datagen,\n",
    "                                                        test_datagen= test_datagen,\n",
    "                                                        image_size=image_size,\n",
    "                                                        ismixup=True\n",
    "                                                        )\n",
    "        #get model path\n",
    "        model_path = get_model_path(model_dir, model_title, model_type, fold_step+1)\n",
    "        #get model\n",
    "        model = get_model(model_type=model_type, image_size=image_size, opt=optimizers.RMSprop(lr=lr), lr=lr,\n",
    "                          activation=params[model_type]['activation'],\n",
    "                         dense=params[model_type]['dense'],\n",
    "                         drop=params[model_type]['drop'])\n",
    "        #train\n",
    "        history = model.fit_generator(\n",
    "            train_gen,\n",
    "            steps_per_epoch=get_steps(X_train.shape[0], batch_size),\n",
    "            epochs=epoch,\n",
    "            validation_data=valid_gen,\n",
    "            validation_steps=get_steps(X_val.shape[0], valid_batch_size),\n",
    "            verbose=1,\n",
    "            callbacks=get_callback(patient, model_path, lr, len(X_train), cosine=False, warmup_epoch=3)\n",
    "        )\n",
    "        \n",
    "        rs_models[model_type+'_foldstep_'+str(fold_step)] = model_path\n",
    "        rs_histories[model_type+'_foldstep_'+str(fold_step)] = history\n",
    "    return rs_models, rs_histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = '../model/'\n",
    "DATA_PATH = '../input'\n",
    "df_train = pd.read_csv(os.path.join(DATA_PATH, 'train.csv'))\n",
    "df_test = pd.read_csv(os.path.join(DATA_PATH, 'test.csv'))\n",
    "df_train[\"class\"] = df_train[\"class\"].astype('str')\n",
    "df_test = df_test[['img_file']]\n",
    "model_title = 'emsemble_final'\n",
    "mymodels = {}\n",
    "myhistories = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient = 25\n",
    "epoch=150\n",
    "lr=0.0001\n",
    "batch_size = 16\n",
    "valid_batch_size = 64\n",
    "n_splits = 5\n",
    "model_type = 'EfficientNetB3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*]-------------------Start train EfficientNetB3 model-------------------\n",
      "[*]-------------------EfficientNetB3 model_foldstep_0-------------------\n",
      ">>image_size: 299\n",
      ">>train_path: ../input/train_crop_299\n",
      ">>test_path: ../input/test_crop_299\n",
      "Found 7914 validated image filenames belonging to 196 classes.\n",
      "Found 7914 validated image filenames belonging to 196 classes.\n",
      "Found 2076 validated image filenames belonging to 196 classes.\n",
      "Found 6150 validated image filenames.\n",
      ">>model path to save: ../model/emsemble_final_EfficientNetB3_fold_1.hdf5\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/function.py:987: calling Graph.create_op (from tensorflow.python.framework.ops) with compute_shapes is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Shapes are always computed; don't use the compute_shapes as it has no effect.\n",
      "Epoch 1/150\n",
      "495/495 [==============================] - 234s 473ms/step - loss: 5.2022 - acc: 0.0160 - f1_m: 0.0000e+00 - val_loss: 4.5868 - val_acc: 0.0739 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_f1_m improved from -inf to 0.00000, saving model to ../model/emsemble_final_EfficientNetB3_fold_1.hdf5\n",
      "Epoch 2/150\n",
      "495/495 [==============================] - 226s 456ms/step - loss: 4.2543 - acc: 0.1222 - f1_m: 0.0094 - val_loss: 2.8935 - val_acc: 0.3220 - val_f1_m: 0.1003\n",
      "\n",
      "Epoch 00002: val_f1_m improved from 0.00000 to 0.10033, saving model to ../model/emsemble_final_EfficientNetB3_fold_1.hdf5\n",
      "Epoch 3/150\n",
      "495/495 [==============================] - 226s 457ms/step - loss: 3.0370 - acc: 0.3201 - f1_m: 0.1221 - val_loss: 1.7892 - val_acc: 0.5417 - val_f1_m: 0.4112\n",
      "\n",
      "Epoch 00003: val_f1_m improved from 0.10033 to 0.41118, saving model to ../model/emsemble_final_EfficientNetB3_fold_1.hdf5\n",
      "Epoch 4/150\n",
      "495/495 [==============================] - 227s 458ms/step - loss: 2.2741 - acc: 0.4809 - f1_m: 0.3431 - val_loss: 1.1635 - val_acc: 0.7156 - val_f1_m: 0.6545\n",
      "\n",
      "Epoch 00004: val_f1_m improved from 0.41118 to 0.65451, saving model to ../model/emsemble_final_EfficientNetB3_fold_1.hdf5\n",
      "Epoch 5/150\n",
      "495/495 [==============================] - 226s 457ms/step - loss: 1.8195 - acc: 0.5962 - f1_m: 0.5098 - val_loss: 0.8854 - val_acc: 0.7576 - val_f1_m: 0.7374\n",
      "\n",
      "Epoch 00005: val_f1_m improved from 0.65451 to 0.73741, saving model to ../model/emsemble_final_EfficientNetB3_fold_1.hdf5\n",
      "Epoch 6/150\n",
      "495/495 [==============================] - 226s 457ms/step - loss: 1.5058 - acc: 0.6730 - f1_m: 0.6152 - val_loss: 0.7158 - val_acc: 0.7898 - val_f1_m: 0.7750\n",
      "\n",
      "Epoch 00006: val_f1_m improved from 0.73741 to 0.77497, saving model to ../model/emsemble_final_EfficientNetB3_fold_1.hdf5\n",
      "Epoch 7/150\n",
      "495/495 [==============================] - 226s 457ms/step - loss: 1.2953 - acc: 0.7327 - f1_m: 0.6969 - val_loss: 0.6310 - val_acc: 0.8220 - val_f1_m: 0.8226\n",
      "\n",
      "Epoch 00007: val_f1_m improved from 0.77497 to 0.82261, saving model to ../model/emsemble_final_EfficientNetB3_fold_1.hdf5\n",
      "Epoch 8/150\n",
      "495/495 [==============================] - 226s 457ms/step - loss: 1.1685 - acc: 0.7690 - f1_m: 0.7378 - val_loss: 0.6641 - val_acc: 0.8092 - val_f1_m: 0.8223\n",
      "\n",
      "Epoch 00008: val_f1_m did not improve from 0.82261\n",
      "Epoch 9/150\n",
      "495/495 [==============================] - 226s 457ms/step - loss: 1.0267 - acc: 0.8149 - f1_m: 0.7876 - val_loss: 0.4859 - val_acc: 0.8523 - val_f1_m: 0.8547\n",
      "\n",
      "Epoch 00009: val_f1_m improved from 0.82261 to 0.85471, saving model to ../model/emsemble_final_EfficientNetB3_fold_1.hdf5\n",
      "Epoch 10/150\n",
      "495/495 [==============================] - 226s 457ms/step - loss: 0.9331 - acc: 0.8384 - f1_m: 0.8191 - val_loss: 0.4495 - val_acc: 0.8731 - val_f1_m: 0.8751\n",
      "\n",
      "Epoch 00010: val_f1_m improved from 0.85471 to 0.87509, saving model to ../model/emsemble_final_EfficientNetB3_fold_1.hdf5\n",
      "Epoch 11/150\n",
      "495/495 [==============================] - 226s 457ms/step - loss: 0.8582 - acc: 0.8587 - f1_m: 0.8397 - val_loss: 0.4850 - val_acc: 0.8617 - val_f1_m: 0.8611\n",
      "\n",
      "Epoch 00011: val_f1_m did not improve from 0.87509\n",
      "Epoch 12/150\n",
      "495/495 [==============================] - 226s 457ms/step - loss: 0.8075 - acc: 0.8787 - f1_m: 0.8612 - val_loss: 0.4712 - val_acc: 0.8836 - val_f1_m: 0.8785\n",
      "\n",
      "Epoch 00012: val_f1_m improved from 0.87509 to 0.87849, saving model to ../model/emsemble_final_EfficientNetB3_fold_1.hdf5\n",
      "Epoch 13/150\n",
      "495/495 [==============================] - 226s 457ms/step - loss: 0.7604 - acc: 0.8924 - f1_m: 0.8790 - val_loss: 0.4309 - val_acc: 0.8807 - val_f1_m: 0.8800\n",
      "\n",
      "Epoch 00013: val_f1_m improved from 0.87849 to 0.87999, saving model to ../model/emsemble_final_EfficientNetB3_fold_1.hdf5\n",
      "Epoch 14/150\n",
      "495/495 [==============================] - 226s 457ms/step - loss: 0.7178 - acc: 0.9101 - f1_m: 0.8935 - val_loss: 0.4198 - val_acc: 0.8958 - val_f1_m: 0.8979\n",
      "\n",
      "Epoch 00014: val_f1_m improved from 0.87999 to 0.89789, saving model to ../model/emsemble_final_EfficientNetB3_fold_1.hdf5\n",
      "Epoch 15/150\n",
      "495/495 [==============================] - 226s 457ms/step - loss: 0.6755 - acc: 0.9219 - f1_m: 0.9041 - val_loss: 0.4718 - val_acc: 0.8769 - val_f1_m: 0.8781\n",
      "\n",
      "Epoch 00015: val_f1_m did not improve from 0.89789\n",
      "Epoch 16/150\n",
      "495/495 [==============================] - 226s 457ms/step - loss: 0.6544 - acc: 0.9288 - f1_m: 0.9187 - val_loss: 0.3327 - val_acc: 0.9084 - val_f1_m: 0.9062\n",
      "\n",
      "Epoch 00016: val_f1_m improved from 0.89789 to 0.90622, saving model to ../model/emsemble_final_EfficientNetB3_fold_1.hdf5\n",
      "Epoch 17/150\n",
      "495/495 [==============================] - 226s 457ms/step - loss: 0.6285 - acc: 0.9362 - f1_m: 0.9238 - val_loss: 0.4588 - val_acc: 0.8807 - val_f1_m: 0.8812\n",
      "\n",
      "Epoch 00017: val_f1_m did not improve from 0.90622\n",
      "Epoch 18/150\n",
      "495/495 [==============================] - 225s 455ms/step - loss: 0.6168 - acc: 0.9390 - f1_m: 0.9271 - val_loss: 0.3790 - val_acc: 0.8864 - val_f1_m: 0.8926\n",
      "\n",
      "Epoch 00018: val_f1_m did not improve from 0.90622\n",
      "Epoch 19/150\n",
      "495/495 [==============================] - 226s 456ms/step - loss: 0.5740 - acc: 0.9488 - f1_m: 0.9368 - val_loss: 0.3091 - val_acc: 0.9280 - val_f1_m: 0.9223\n",
      "\n",
      "Epoch 00019: val_f1_m improved from 0.90622 to 0.92228, saving model to ../model/emsemble_final_EfficientNetB3_fold_1.hdf5\n",
      "Epoch 20/150\n",
      "495/495 [==============================] - 225s 455ms/step - loss: 0.5850 - acc: 0.9471 - f1_m: 0.9356 - val_loss: 0.4333 - val_acc: 0.8989 - val_f1_m: 0.8931\n",
      "\n",
      "Epoch 00020: val_f1_m did not improve from 0.92228\n",
      "Epoch 21/150\n",
      "495/495 [==============================] - 226s 458ms/step - loss: 0.5606 - acc: 0.9536 - f1_m: 0.9432 - val_loss: 0.4529 - val_acc: 0.8788 - val_f1_m: 0.8778\n",
      "\n",
      "Epoch 00021: val_f1_m did not improve from 0.92228\n",
      "Epoch 22/150\n",
      "495/495 [==============================] - 226s 457ms/step - loss: 0.5556 - acc: 0.9555 - f1_m: 0.9451 - val_loss: 0.4412 - val_acc: 0.9015 - val_f1_m: 0.8925\n",
      "\n",
      "Epoch 00022: val_f1_m did not improve from 0.92228\n",
      "Epoch 23/150\n",
      "495/495 [==============================] - 225s 455ms/step - loss: 0.5224 - acc: 0.9672 - f1_m: 0.9552 - val_loss: 0.3278 - val_acc: 0.9167 - val_f1_m: 0.9177\n",
      "\n",
      "Epoch 00023: val_f1_m did not improve from 0.92228\n",
      "Epoch 24/150\n",
      "495/495 [==============================] - 225s 455ms/step - loss: 0.5233 - acc: 0.9663 - f1_m: 0.9557 - val_loss: 0.3652 - val_acc: 0.9237 - val_f1_m: 0.9090\n",
      "\n",
      "Epoch 00024: val_f1_m did not improve from 0.92228\n",
      "Epoch 25/150\n",
      "495/495 [==============================] - 226s 457ms/step - loss: 0.5104 - acc: 0.9689 - f1_m: 0.9572 - val_loss: 0.4221 - val_acc: 0.9148 - val_f1_m: 0.9061\n",
      "\n",
      "Epoch 00025: val_f1_m did not improve from 0.92228\n",
      "Epoch 26/150\n",
      "495/495 [==============================] - 226s 457ms/step - loss: 0.5044 - acc: 0.9695 - f1_m: 0.9593 - val_loss: 0.3995 - val_acc: 0.9129 - val_f1_m: 0.9037\n",
      "\n",
      "Epoch 00026: val_f1_m did not improve from 0.92228\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Epoch 27/150\n",
      "495/495 [==============================] - 226s 457ms/step - loss: 0.4788 - acc: 0.9769 - f1_m: 0.9672 - val_loss: 0.3064 - val_acc: 0.9223 - val_f1_m: 0.9252\n",
      "\n",
      "Epoch 00027: val_f1_m improved from 0.92228 to 0.92524, saving model to ../model/emsemble_final_EfficientNetB3_fold_1.hdf5\n",
      "Epoch 28/150\n",
      "495/495 [==============================] - 226s 457ms/step - loss: 0.4777 - acc: 0.9766 - f1_m: 0.9679 - val_loss: 0.3036 - val_acc: 0.9141 - val_f1_m: 0.9118\n",
      "\n",
      "Epoch 00028: val_f1_m did not improve from 0.92524\n",
      "Epoch 29/150\n",
      "495/495 [==============================] - 227s 458ms/step - loss: 0.4571 - acc: 0.9814 - f1_m: 0.9734 - val_loss: 0.3376 - val_acc: 0.9110 - val_f1_m: 0.9032\n",
      "\n",
      "Epoch 00029: val_f1_m did not improve from 0.92524\n",
      "Epoch 30/150\n",
      "495/495 [==============================] - 226s 457ms/step - loss: 0.4586 - acc: 0.9792 - f1_m: 0.9723 - val_loss: 0.3741 - val_acc: 0.9186 - val_f1_m: 0.9199\n",
      "\n",
      "Epoch 00030: val_f1_m did not improve from 0.92524\n",
      "Epoch 31/150\n",
      "495/495 [==============================] - 226s 457ms/step - loss: 0.4467 - acc: 0.9819 - f1_m: 0.9748 - val_loss: 0.4196 - val_acc: 0.9072 - val_f1_m: 0.9020\n",
      "\n",
      "Epoch 00031: val_f1_m did not improve from 0.92524\n",
      "Epoch 32/150\n",
      "495/495 [==============================] - 226s 457ms/step - loss: 0.4471 - acc: 0.9807 - f1_m: 0.9751 - val_loss: 0.3723 - val_acc: 0.9141 - val_f1_m: 0.9070\n",
      "\n",
      "Epoch 00032: val_f1_m did not improve from 0.92524\n",
      "Epoch 33/150\n",
      "495/495 [==============================] - 226s 457ms/step - loss: 0.4408 - acc: 0.9838 - f1_m: 0.9768 - val_loss: 0.3813 - val_acc: 0.9129 - val_f1_m: 0.9052\n",
      "\n",
      "Epoch 00033: val_f1_m did not improve from 0.92524\n",
      "Epoch 34/150\n",
      "495/495 [==============================] - 226s 457ms/step - loss: 0.4390 - acc: 0.9841 - f1_m: 0.9767 - val_loss: 0.3543 - val_acc: 0.9356 - val_f1_m: 0.9319\n",
      "\n",
      "Epoch 00034: val_f1_m improved from 0.92524 to 0.93192, saving model to ../model/emsemble_final_EfficientNetB3_fold_1.hdf5\n",
      "Epoch 35/150\n",
      "495/495 [==============================] - 226s 458ms/step - loss: 0.4364 - acc: 0.9836 - f1_m: 0.9769 - val_loss: 0.3080 - val_acc: 0.9223 - val_f1_m: 0.9176\n",
      "\n",
      "Epoch 00035: val_f1_m did not improve from 0.93192\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 36/150\n",
      "495/495 [==============================] - 226s 457ms/step - loss: 0.4304 - acc: 0.9864 - f1_m: 0.9799 - val_loss: 0.3903 - val_acc: 0.9122 - val_f1_m: 0.9096\n",
      "\n",
      "Epoch 00036: val_f1_m did not improve from 0.93192\n",
      "Epoch 37/150\n",
      "495/495 [==============================] - 226s 457ms/step - loss: 0.4194 - acc: 0.9879 - f1_m: 0.9828 - val_loss: 0.2951 - val_acc: 0.9394 - val_f1_m: 0.9391\n",
      "\n",
      "Epoch 00037: val_f1_m improved from 0.93192 to 0.93907, saving model to ../model/emsemble_final_EfficientNetB3_fold_1.hdf5\n",
      "Epoch 38/150\n",
      "495/495 [==============================] - 226s 458ms/step - loss: 0.4182 - acc: 0.9871 - f1_m: 0.9835 - val_loss: 0.3984 - val_acc: 0.8996 - val_f1_m: 0.8978\n",
      "\n",
      "Epoch 00038: val_f1_m did not improve from 0.93907\n",
      "Epoch 39/150\n",
      "495/495 [==============================] - 226s 457ms/step - loss: 0.4221 - acc: 0.9847 - f1_m: 0.9811 - val_loss: 0.3652 - val_acc: 0.9280 - val_f1_m: 0.9221\n",
      "\n",
      "Epoch 00039: val_f1_m did not improve from 0.93907\n",
      "Epoch 40/150\n",
      "495/495 [==============================] - 226s 458ms/step - loss: 0.4162 - acc: 0.9879 - f1_m: 0.9826 - val_loss: 0.3284 - val_acc: 0.9237 - val_f1_m: 0.9143\n",
      "\n",
      "Epoch 00040: val_f1_m did not improve from 0.93907\n",
      "Epoch 41/150\n",
      "495/495 [==============================] - 225s 455ms/step - loss: 0.4091 - acc: 0.9909 - f1_m: 0.9840 - val_loss: 0.3326 - val_acc: 0.9261 - val_f1_m: 0.9146\n",
      "\n",
      "Epoch 00041: val_f1_m did not improve from 0.93907\n",
      "Epoch 42/150\n",
      "495/495 [==============================] - 226s 457ms/step - loss: 0.4100 - acc: 0.9890 - f1_m: 0.9830 - val_loss: 0.3091 - val_acc: 0.9375 - val_f1_m: 0.9282\n",
      "\n",
      "Epoch 00042: val_f1_m did not improve from 0.93907\n",
      "Epoch 43/150\n",
      "495/495 [==============================] - 226s 457ms/step - loss: 0.4143 - acc: 0.9894 - f1_m: 0.9845 - val_loss: 0.4000 - val_acc: 0.9186 - val_f1_m: 0.9150\n",
      "\n",
      "Epoch 00043: val_f1_m did not improve from 0.93907\n",
      "Epoch 44/150\n",
      "495/495 [==============================] - 226s 457ms/step - loss: 0.4065 - acc: 0.9893 - f1_m: 0.9854 - val_loss: 0.3763 - val_acc: 0.9218 - val_f1_m: 0.9165\n",
      "\n",
      "Epoch 00044: val_f1_m did not improve from 0.93907\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 45/150\n",
      "495/495 [==============================] - 226s 457ms/step - loss: 0.4016 - acc: 0.9904 - f1_m: 0.9857 - val_loss: 0.3630 - val_acc: 0.9299 - val_f1_m: 0.9258\n",
      "\n",
      "Epoch 00045: val_f1_m did not improve from 0.93907\n",
      "Epoch 46/150\n",
      "495/495 [==============================] - 227s 458ms/step - loss: 0.4040 - acc: 0.9905 - f1_m: 0.9861 - val_loss: 0.3655 - val_acc: 0.9261 - val_f1_m: 0.9128\n",
      "\n",
      "Epoch 00046: val_f1_m did not improve from 0.93907\n",
      "Epoch 47/150\n",
      "495/495 [==============================] - 226s 457ms/step - loss: 0.3984 - acc: 0.9920 - f1_m: 0.9864 - val_loss: 0.3337 - val_acc: 0.9242 - val_f1_m: 0.9201\n",
      "\n",
      "Epoch 00047: val_f1_m did not improve from 0.93907\n",
      "Epoch 48/150\n",
      "495/495 [==============================] - 225s 455ms/step - loss: 0.3977 - acc: 0.9917 - f1_m: 0.9870 - val_loss: 0.2840 - val_acc: 0.9370 - val_f1_m: 0.9339\n",
      "\n",
      "Epoch 00048: val_f1_m did not improve from 0.93907\n",
      "Epoch 49/150\n",
      "495/495 [==============================] - 226s 457ms/step - loss: 0.3934 - acc: 0.9922 - f1_m: 0.9873 - val_loss: 0.3954 - val_acc: 0.9223 - val_f1_m: 0.9154\n",
      "\n",
      "Epoch 00049: val_f1_m did not improve from 0.93907\n",
      "Epoch 50/150\n",
      "495/495 [==============================] - 226s 458ms/step - loss: 0.4016 - acc: 0.9910 - f1_m: 0.9863 - val_loss: 0.3968 - val_acc: 0.9186 - val_f1_m: 0.9181\n",
      "\n",
      "Epoch 00050: val_f1_m did not improve from 0.93907\n",
      "Epoch 51/150\n",
      "495/495 [==============================] - 225s 455ms/step - loss: 0.3983 - acc: 0.9910 - f1_m: 0.9864 - val_loss: 0.3059 - val_acc: 0.9299 - val_f1_m: 0.9205\n",
      "\n",
      "Epoch 00051: val_f1_m did not improve from 0.93907\n",
      "Epoch 52/150\n",
      "495/495 [==============================] - 226s 457ms/step - loss: 0.3953 - acc: 0.9922 - f1_m: 0.9875 - val_loss: 0.3394 - val_acc: 0.9332 - val_f1_m: 0.9221\n",
      "\n",
      "Epoch 00052: val_f1_m did not improve from 0.93907\n",
      "Epoch 53/150\n",
      "495/495 [==============================] - 226s 457ms/step - loss: 0.3929 - acc: 0.9923 - f1_m: 0.9877 - val_loss: 0.4102 - val_acc: 0.9186 - val_f1_m: 0.9115\n",
      "\n",
      "Epoch 00053: val_f1_m did not improve from 0.93907\n",
      "Epoch 54/150\n",
      "495/495 [==============================] - 226s 458ms/step - loss: 0.3901 - acc: 0.9927 - f1_m: 0.9878 - val_loss: 0.3522 - val_acc: 0.9280 - val_f1_m: 0.9247\n",
      "\n",
      "Epoch 00054: val_f1_m did not improve from 0.93907\n",
      "Epoch 55/150\n",
      "495/495 [==============================] - 226s 457ms/step - loss: 0.3956 - acc: 0.9914 - f1_m: 0.9873 - val_loss: 0.3130 - val_acc: 0.9337 - val_f1_m: 0.9248\n",
      "\n",
      "Epoch 00055: val_f1_m did not improve from 0.93907\n",
      "\n",
      "Epoch 00055: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "Epoch 56/150\n",
      "495/495 [==============================] - 226s 457ms/step - loss: 0.3927 - acc: 0.9938 - f1_m: 0.9878 - val_loss: 0.4159 - val_acc: 0.9122 - val_f1_m: 0.9089\n",
      "\n",
      "Epoch 00056: val_f1_m did not improve from 0.93907\n",
      "Epoch 57/150\n",
      "495/495 [==============================] - 226s 457ms/step - loss: 0.3940 - acc: 0.9915 - f1_m: 0.9874 - val_loss: 0.2664 - val_acc: 0.9451 - val_f1_m: 0.9381\n",
      "\n",
      "Epoch 00057: val_f1_m did not improve from 0.93907\n",
      "Epoch 58/150\n",
      "495/495 [==============================] - 226s 457ms/step - loss: 0.3932 - acc: 0.9922 - f1_m: 0.9874 - val_loss: 0.3712 - val_acc: 0.9299 - val_f1_m: 0.9197\n",
      "\n",
      "Epoch 00058: val_f1_m did not improve from 0.93907\n",
      "Epoch 59/150\n",
      "495/495 [==============================] - 227s 458ms/step - loss: 0.3891 - acc: 0.9934 - f1_m: 0.9893 - val_loss: 0.3285 - val_acc: 0.9318 - val_f1_m: 0.9166\n",
      "\n",
      "Epoch 00059: val_f1_m did not improve from 0.93907\n",
      "Epoch 60/150\n",
      "495/495 [==============================] - 226s 457ms/step - loss: 0.3919 - acc: 0.9931 - f1_m: 0.9879 - val_loss: 0.4131 - val_acc: 0.9084 - val_f1_m: 0.9041\n",
      "\n",
      "Epoch 00060: val_f1_m did not improve from 0.93907\n",
      "Epoch 61/150\n",
      "495/495 [==============================] - 227s 458ms/step - loss: 0.3930 - acc: 0.9926 - f1_m: 0.9886 - val_loss: 0.3681 - val_acc: 0.9299 - val_f1_m: 0.9237\n",
      "\n",
      "Epoch 00061: val_f1_m did not improve from 0.93907\n",
      "Epoch 62/150\n",
      "495/495 [==============================] - 227s 458ms/step - loss: 0.3978 - acc: 0.9905 - f1_m: 0.9873 - val_loss: 0.3809 - val_acc: 0.9167 - val_f1_m: 0.9087\n",
      "\n",
      "Epoch 00062: val_f1_m did not improve from 0.93907\n",
      "Epoch 00062: early stopping\n",
      "[*]-------------------EfficientNetB3 model_foldstep_1-------------------\n",
      ">>image_size: 299\n",
      ">>train_path: ../input/train_crop_299\n",
      ">>test_path: ../input/test_crop_299\n",
      "Found 7952 validated image filenames belonging to 196 classes.\n",
      "Found 7952 validated image filenames belonging to 196 classes.\n",
      "Found 2038 validated image filenames belonging to 196 classes.\n",
      "Found 6150 validated image filenames.\n",
      ">>model path to save: ../model/emsemble_final_EfficientNetB3_fold_2.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "497/497 [==============================] - 238s 480ms/step - loss: 5.2299 - acc: 0.0161 - f1_m: 0.0000e+00 - val_loss: 4.7833 - val_acc: 0.0820 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_f1_m improved from -inf to 0.00000, saving model to ../model/emsemble_final_EfficientNetB3_fold_2.hdf5\n",
      "Epoch 2/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 4.3101 - acc: 0.1187 - f1_m: 0.0104 - val_loss: 2.8653 - val_acc: 0.3730 - val_f1_m: 0.0746\n",
      "\n",
      "Epoch 00002: val_f1_m improved from 0.00000 to 0.07455, saving model to ../model/emsemble_final_EfficientNetB3_fold_2.hdf5\n",
      "Epoch 3/150\n",
      "497/497 [==============================] - 226s 454ms/step - loss: 3.0834 - acc: 0.3122 - f1_m: 0.1115 - val_loss: 1.7833 - val_acc: 0.5371 - val_f1_m: 0.3912\n",
      "\n",
      "Epoch 00003: val_f1_m improved from 0.07455 to 0.39116, saving model to ../model/emsemble_final_EfficientNetB3_fold_2.hdf5\n",
      "Epoch 4/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 2.2956 - acc: 0.4843 - f1_m: 0.3256 - val_loss: 1.1412 - val_acc: 0.6833 - val_f1_m: 0.6303\n",
      "\n",
      "Epoch 00004: val_f1_m improved from 0.39116 to 0.63031, saving model to ../model/emsemble_final_EfficientNetB3_fold_2.hdf5\n",
      "Epoch 5/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 1.7995 - acc: 0.5954 - f1_m: 0.5010 - val_loss: 0.7374 - val_acc: 0.8105 - val_f1_m: 0.7782\n",
      "\n",
      "Epoch 00005: val_f1_m improved from 0.63031 to 0.77820, saving model to ../model/emsemble_final_EfficientNetB3_fold_2.hdf5\n",
      "Epoch 6/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 1.4847 - acc: 0.6768 - f1_m: 0.6179 - val_loss: 0.7734 - val_acc: 0.7715 - val_f1_m: 0.7738\n",
      "\n",
      "Epoch 00006: val_f1_m did not improve from 0.77820\n",
      "Epoch 7/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 1.2665 - acc: 0.7465 - f1_m: 0.7040 - val_loss: 0.5518 - val_acc: 0.8340 - val_f1_m: 0.8333\n",
      "\n",
      "Epoch 00007: val_f1_m improved from 0.77820 to 0.83335, saving model to ../model/emsemble_final_EfficientNetB3_fold_2.hdf5\n",
      "Epoch 8/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 1.1401 - acc: 0.7752 - f1_m: 0.7425 - val_loss: 0.5659 - val_acc: 0.8347 - val_f1_m: 0.8435\n",
      "\n",
      "Epoch 00008: val_f1_m improved from 0.83335 to 0.84349, saving model to ../model/emsemble_final_EfficientNetB3_fold_2.hdf5\n",
      "Epoch 9/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 1.0039 - acc: 0.8138 - f1_m: 0.7925 - val_loss: 0.5270 - val_acc: 0.8418 - val_f1_m: 0.8524\n",
      "\n",
      "Epoch 00009: val_f1_m improved from 0.84349 to 0.85241, saving model to ../model/emsemble_final_EfficientNetB3_fold_2.hdf5\n",
      "Epoch 10/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.9242 - acc: 0.8398 - f1_m: 0.8237 - val_loss: 0.5201 - val_acc: 0.8477 - val_f1_m: 0.8476\n",
      "\n",
      "Epoch 00010: val_f1_m did not improve from 0.85241\n",
      "Epoch 11/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.8444 - acc: 0.8716 - f1_m: 0.8508 - val_loss: 0.4018 - val_acc: 0.8789 - val_f1_m: 0.8857\n",
      "\n",
      "Epoch 00011: val_f1_m improved from 0.85241 to 0.88566, saving model to ../model/emsemble_final_EfficientNetB3_fold_2.hdf5\n",
      "Epoch 12/150\n",
      "497/497 [==============================] - 226s 454ms/step - loss: 0.7932 - acc: 0.8763 - f1_m: 0.8602 - val_loss: 0.4044 - val_acc: 0.8725 - val_f1_m: 0.8702\n",
      "\n",
      "Epoch 00012: val_f1_m did not improve from 0.88566\n",
      "Epoch 13/150\n",
      "497/497 [==============================] - 226s 454ms/step - loss: 0.7546 - acc: 0.8969 - f1_m: 0.8818 - val_loss: 0.4151 - val_acc: 0.8691 - val_f1_m: 0.8697\n",
      "\n",
      "Epoch 00013: val_f1_m did not improve from 0.88566\n",
      "Epoch 14/150\n",
      "497/497 [==============================] - 226s 454ms/step - loss: 0.7047 - acc: 0.9093 - f1_m: 0.8975 - val_loss: 0.3857 - val_acc: 0.8945 - val_f1_m: 0.8921\n",
      "\n",
      "Epoch 00014: val_f1_m improved from 0.88566 to 0.89207, saving model to ../model/emsemble_final_EfficientNetB3_fold_2.hdf5\n",
      "Epoch 15/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.6789 - acc: 0.9159 - f1_m: 0.9055 - val_loss: 0.3590 - val_acc: 0.9023 - val_f1_m: 0.8975\n",
      "\n",
      "Epoch 00015: val_f1_m improved from 0.89207 to 0.89755, saving model to ../model/emsemble_final_EfficientNetB3_fold_2.hdf5\n",
      "Epoch 16/150\n",
      "497/497 [==============================] - 226s 454ms/step - loss: 0.6589 - acc: 0.9230 - f1_m: 0.9129 - val_loss: 0.3301 - val_acc: 0.9143 - val_f1_m: 0.9120\n",
      "\n",
      "Epoch 00016: val_f1_m improved from 0.89755 to 0.91200, saving model to ../model/emsemble_final_EfficientNetB3_fold_2.hdf5\n",
      "Epoch 17/150\n",
      "497/497 [==============================] - 226s 454ms/step - loss: 0.6241 - acc: 0.9364 - f1_m: 0.9240 - val_loss: 0.3502 - val_acc: 0.9082 - val_f1_m: 0.9104\n",
      "\n",
      "Epoch 00017: val_f1_m did not improve from 0.91200\n",
      "Epoch 18/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.6064 - acc: 0.9406 - f1_m: 0.9288 - val_loss: 0.3663 - val_acc: 0.9062 - val_f1_m: 0.8933\n",
      "\n",
      "Epoch 00018: val_f1_m did not improve from 0.91200\n",
      "Epoch 19/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.5719 - acc: 0.9505 - f1_m: 0.9384 - val_loss: 0.3581 - val_acc: 0.9004 - val_f1_m: 0.9094\n",
      "\n",
      "Epoch 00019: val_f1_m did not improve from 0.91200\n",
      "Epoch 20/150\n",
      "497/497 [==============================] - 226s 454ms/step - loss: 0.5775 - acc: 0.9477 - f1_m: 0.9381 - val_loss: 0.3445 - val_acc: 0.9203 - val_f1_m: 0.9170\n",
      "\n",
      "Epoch 00020: val_f1_m improved from 0.91200 to 0.91703, saving model to ../model/emsemble_final_EfficientNetB3_fold_2.hdf5\n",
      "Epoch 21/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.5544 - acc: 0.9570 - f1_m: 0.9455 - val_loss: 0.3588 - val_acc: 0.9102 - val_f1_m: 0.9066\n",
      "\n",
      "Epoch 00021: val_f1_m did not improve from 0.91703\n",
      "Epoch 22/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.5473 - acc: 0.9569 - f1_m: 0.9476 - val_loss: 0.3666 - val_acc: 0.9141 - val_f1_m: 0.9109\n",
      "\n",
      "Epoch 00022: val_f1_m did not improve from 0.91703\n",
      "Epoch 23/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.5277 - acc: 0.9627 - f1_m: 0.9516 - val_loss: 0.2994 - val_acc: 0.9258 - val_f1_m: 0.9266\n",
      "\n",
      "Epoch 00023: val_f1_m improved from 0.91703 to 0.92655, saving model to ../model/emsemble_final_EfficientNetB3_fold_2.hdf5\n",
      "Epoch 24/150\n",
      "497/497 [==============================] - 226s 454ms/step - loss: 0.5291 - acc: 0.9628 - f1_m: 0.9517 - val_loss: 0.3719 - val_acc: 0.8984 - val_f1_m: 0.8935\n",
      "\n",
      "Epoch 00024: val_f1_m did not improve from 0.92655\n",
      "Epoch 25/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.5092 - acc: 0.9663 - f1_m: 0.9573 - val_loss: 0.3526 - val_acc: 0.9199 - val_f1_m: 0.9124\n",
      "\n",
      "Epoch 00025: val_f1_m did not improve from 0.92655\n",
      "Epoch 26/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.5044 - acc: 0.9679 - f1_m: 0.9590 - val_loss: 0.4329 - val_acc: 0.8848 - val_f1_m: 0.8843\n",
      "\n",
      "Epoch 00026: val_f1_m did not improve from 0.92655\n",
      "Epoch 27/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.4967 - acc: 0.9703 - f1_m: 0.9632 - val_loss: 0.3372 - val_acc: 0.9258 - val_f1_m: 0.9210\n",
      "\n",
      "Epoch 00027: val_f1_m did not improve from 0.92655\n",
      "Epoch 28/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.4907 - acc: 0.9718 - f1_m: 0.9620 - val_loss: 0.2810 - val_acc: 0.9203 - val_f1_m: 0.9230\n",
      "\n",
      "Epoch 00028: val_f1_m did not improve from 0.92655\n",
      "Epoch 29/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.4812 - acc: 0.9746 - f1_m: 0.9674 - val_loss: 0.4044 - val_acc: 0.8965 - val_f1_m: 0.8868\n",
      "\n",
      "Epoch 00029: val_f1_m did not improve from 0.92655\n",
      "Epoch 30/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.4741 - acc: 0.9761 - f1_m: 0.9683 - val_loss: 0.2998 - val_acc: 0.9336 - val_f1_m: 0.9311\n",
      "\n",
      "Epoch 00030: val_f1_m improved from 0.92655 to 0.93111, saving model to ../model/emsemble_final_EfficientNetB3_fold_2.hdf5\n",
      "Epoch 31/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.4641 - acc: 0.9764 - f1_m: 0.9698 - val_loss: 0.2798 - val_acc: 0.9375 - val_f1_m: 0.9368\n",
      "\n",
      "Epoch 00031: val_f1_m improved from 0.93111 to 0.93677, saving model to ../model/emsemble_final_EfficientNetB3_fold_2.hdf5\n",
      "Epoch 32/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.4619 - acc: 0.9789 - f1_m: 0.9701 - val_loss: 0.4421 - val_acc: 0.9004 - val_f1_m: 0.8912\n",
      "\n",
      "Epoch 00032: val_f1_m did not improve from 0.93677\n",
      "Epoch 33/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.4539 - acc: 0.9825 - f1_m: 0.9742 - val_loss: 0.3569 - val_acc: 0.9199 - val_f1_m: 0.9055\n",
      "\n",
      "Epoch 00033: val_f1_m did not improve from 0.93677\n",
      "Epoch 34/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.4470 - acc: 0.9806 - f1_m: 0.9747 - val_loss: 0.3139 - val_acc: 0.9375 - val_f1_m: 0.9334\n",
      "\n",
      "Epoch 00034: val_f1_m did not improve from 0.93677\n",
      "Epoch 35/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.4471 - acc: 0.9829 - f1_m: 0.9762 - val_loss: 0.3324 - val_acc: 0.9238 - val_f1_m: 0.9198\n",
      "\n",
      "Epoch 00035: val_f1_m did not improve from 0.93677\n",
      "Epoch 36/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.4414 - acc: 0.9838 - f1_m: 0.9755 - val_loss: 0.3112 - val_acc: 0.9303 - val_f1_m: 0.9258\n",
      "\n",
      "Epoch 00036: val_f1_m did not improve from 0.93677\n",
      "Epoch 37/150\n",
      "497/497 [==============================] - 226s 454ms/step - loss: 0.4330 - acc: 0.9843 - f1_m: 0.9779 - val_loss: 0.3942 - val_acc: 0.9102 - val_f1_m: 0.9149\n",
      "\n",
      "Epoch 00037: val_f1_m did not improve from 0.93677\n",
      "Epoch 38/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.4300 - acc: 0.9858 - f1_m: 0.9797 - val_loss: 0.3228 - val_acc: 0.9375 - val_f1_m: 0.9169\n",
      "\n",
      "Epoch 00038: val_f1_m did not improve from 0.93677\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Epoch 39/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.4190 - acc: 0.9878 - f1_m: 0.9814 - val_loss: 0.3157 - val_acc: 0.9395 - val_f1_m: 0.9292\n",
      "\n",
      "Epoch 00039: val_f1_m did not improve from 0.93677\n",
      "Epoch 40/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.4092 - acc: 0.9908 - f1_m: 0.9844 - val_loss: 0.3015 - val_acc: 0.9363 - val_f1_m: 0.9237\n",
      "\n",
      "Epoch 00040: val_f1_m did not improve from 0.93677\n",
      "Epoch 41/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.4076 - acc: 0.9889 - f1_m: 0.9842 - val_loss: 0.3327 - val_acc: 0.9297 - val_f1_m: 0.9157\n",
      "\n",
      "Epoch 00041: val_f1_m did not improve from 0.93677\n",
      "Epoch 42/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.3977 - acc: 0.9911 - f1_m: 0.9865 - val_loss: 0.3387 - val_acc: 0.9355 - val_f1_m: 0.9262\n",
      "\n",
      "Epoch 00042: val_f1_m did not improve from 0.93677\n",
      "Epoch 43/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.4038 - acc: 0.9909 - f1_m: 0.9852 - val_loss: 0.2803 - val_acc: 0.9492 - val_f1_m: 0.9316\n",
      "\n",
      "Epoch 00043: val_f1_m did not improve from 0.93677\n",
      "Epoch 44/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.3969 - acc: 0.9908 - f1_m: 0.9865 - val_loss: 0.2904 - val_acc: 0.9482 - val_f1_m: 0.9347\n",
      "\n",
      "Epoch 00044: val_f1_m did not improve from 0.93677\n",
      "Epoch 45/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.3926 - acc: 0.9912 - f1_m: 0.9860 - val_loss: 0.3766 - val_acc: 0.9258 - val_f1_m: 0.9204\n",
      "\n",
      "Epoch 00045: val_f1_m did not improve from 0.93677\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 46/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.3917 - acc: 0.9930 - f1_m: 0.9880 - val_loss: 0.3042 - val_acc: 0.9414 - val_f1_m: 0.9274\n",
      "\n",
      "Epoch 00046: val_f1_m did not improve from 0.93677\n",
      "Epoch 47/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.3869 - acc: 0.9927 - f1_m: 0.9897 - val_loss: 0.2671 - val_acc: 0.9492 - val_f1_m: 0.9381\n",
      "\n",
      "Epoch 00047: val_f1_m improved from 0.93677 to 0.93810, saving model to ../model/emsemble_final_EfficientNetB3_fold_2.hdf5\n",
      "Epoch 48/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.3841 - acc: 0.9930 - f1_m: 0.9888 - val_loss: 0.2831 - val_acc: 0.9502 - val_f1_m: 0.9346\n",
      "\n",
      "Epoch 00048: val_f1_m did not improve from 0.93810\n",
      "Epoch 49/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.3807 - acc: 0.9943 - f1_m: 0.9897 - val_loss: 0.3896 - val_acc: 0.9180 - val_f1_m: 0.9149\n",
      "\n",
      "Epoch 00049: val_f1_m did not improve from 0.93810\n",
      "Epoch 50/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.3883 - acc: 0.9933 - f1_m: 0.9889 - val_loss: 0.2502 - val_acc: 0.9492 - val_f1_m: 0.9430\n",
      "\n",
      "Epoch 00050: val_f1_m improved from 0.93810 to 0.94305, saving model to ../model/emsemble_final_EfficientNetB3_fold_2.hdf5\n",
      "Epoch 51/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.3802 - acc: 0.9938 - f1_m: 0.9901 - val_loss: 0.3457 - val_acc: 0.9375 - val_f1_m: 0.9207\n",
      "\n",
      "Epoch 00051: val_f1_m did not improve from 0.94305\n",
      "Epoch 52/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.3842 - acc: 0.9922 - f1_m: 0.9878 - val_loss: 0.2341 - val_acc: 0.9562 - val_f1_m: 0.9433\n",
      "\n",
      "Epoch 00052: val_f1_m improved from 0.94305 to 0.94333, saving model to ../model/emsemble_final_EfficientNetB3_fold_2.hdf5\n",
      "Epoch 53/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.3765 - acc: 0.9945 - f1_m: 0.9896 - val_loss: 0.3054 - val_acc: 0.9414 - val_f1_m: 0.9294\n",
      "\n",
      "Epoch 00053: val_f1_m did not improve from 0.94333\n",
      "Epoch 54/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.3725 - acc: 0.9947 - f1_m: 0.9904 - val_loss: 0.2778 - val_acc: 0.9434 - val_f1_m: 0.9256\n",
      "\n",
      "Epoch 00054: val_f1_m did not improve from 0.94333\n",
      "Epoch 55/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.3758 - acc: 0.9961 - f1_m: 0.9927 - val_loss: 0.3199 - val_acc: 0.9375 - val_f1_m: 0.9330\n",
      "\n",
      "Epoch 00055: val_f1_m did not improve from 0.94333\n",
      "Epoch 56/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.3768 - acc: 0.9947 - f1_m: 0.9907 - val_loss: 0.3327 - val_acc: 0.9442 - val_f1_m: 0.9310\n",
      "\n",
      "Epoch 00056: val_f1_m did not improve from 0.94333\n",
      "Epoch 57/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.3737 - acc: 0.9947 - f1_m: 0.9917 - val_loss: 0.3042 - val_acc: 0.9414 - val_f1_m: 0.9316\n",
      "\n",
      "Epoch 00057: val_f1_m did not improve from 0.94333\n",
      "Epoch 58/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.3725 - acc: 0.9950 - f1_m: 0.9903 - val_loss: 0.2401 - val_acc: 0.9492 - val_f1_m: 0.9452\n",
      "\n",
      "Epoch 00058: val_f1_m improved from 0.94333 to 0.94522, saving model to ../model/emsemble_final_EfficientNetB3_fold_2.hdf5\n",
      "Epoch 59/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.3741 - acc: 0.9948 - f1_m: 0.9917 - val_loss: 0.3073 - val_acc: 0.9395 - val_f1_m: 0.9293\n",
      "\n",
      "Epoch 00059: val_f1_m did not improve from 0.94522\n",
      "\n",
      "Epoch 00059: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 60/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.3765 - acc: 0.9937 - f1_m: 0.9898 - val_loss: 0.3616 - val_acc: 0.9283 - val_f1_m: 0.9121\n",
      "\n",
      "Epoch 00060: val_f1_m did not improve from 0.94522\n",
      "Epoch 61/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.3737 - acc: 0.9956 - f1_m: 0.9910 - val_loss: 0.3360 - val_acc: 0.9395 - val_f1_m: 0.9292\n",
      "\n",
      "Epoch 00061: val_f1_m did not improve from 0.94522\n",
      "Epoch 62/150\n",
      "497/497 [==============================] - 226s 454ms/step - loss: 0.3735 - acc: 0.9945 - f1_m: 0.9910 - val_loss: 0.2874 - val_acc: 0.9395 - val_f1_m: 0.9386\n",
      "\n",
      "Epoch 00062: val_f1_m did not improve from 0.94522\n",
      "Epoch 63/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.3667 - acc: 0.9962 - f1_m: 0.9924 - val_loss: 0.2927 - val_acc: 0.9395 - val_f1_m: 0.9292\n",
      "\n",
      "Epoch 00063: val_f1_m did not improve from 0.94522\n",
      "Epoch 64/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.3687 - acc: 0.9942 - f1_m: 0.9916 - val_loss: 0.2993 - val_acc: 0.9363 - val_f1_m: 0.9292\n",
      "\n",
      "Epoch 00064: val_f1_m did not improve from 0.94522\n",
      "Epoch 65/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.3621 - acc: 0.9962 - f1_m: 0.9933 - val_loss: 0.3905 - val_acc: 0.9258 - val_f1_m: 0.9209\n",
      "\n",
      "Epoch 00065: val_f1_m did not improve from 0.94522\n",
      "Epoch 66/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "497/497 [==============================] - 226s 455ms/step - loss: 0.3689 - acc: 0.9959 - f1_m: 0.9917 - val_loss: 0.2223 - val_acc: 0.9609 - val_f1_m: 0.9562\n",
      "\n",
      "Epoch 00066: val_f1_m improved from 0.94522 to 0.95620, saving model to ../model/emsemble_final_EfficientNetB3_fold_2.hdf5\n",
      "Epoch 67/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.3748 - acc: 0.9942 - f1_m: 0.9908 - val_loss: 0.3125 - val_acc: 0.9336 - val_f1_m: 0.9164\n",
      "\n",
      "Epoch 00067: val_f1_m did not improve from 0.95620\n",
      "Epoch 68/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.3665 - acc: 0.9947 - f1_m: 0.9925 - val_loss: 0.2941 - val_acc: 0.9363 - val_f1_m: 0.9355\n",
      "\n",
      "Epoch 00068: val_f1_m did not improve from 0.95620\n",
      "Epoch 69/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.3698 - acc: 0.9959 - f1_m: 0.9917 - val_loss: 0.2602 - val_acc: 0.9512 - val_f1_m: 0.9375\n",
      "\n",
      "Epoch 00069: val_f1_m did not improve from 0.95620\n",
      "Epoch 70/150\n",
      "497/497 [==============================] - 226s 456ms/step - loss: 0.3663 - acc: 0.9955 - f1_m: 0.9935 - val_loss: 0.3162 - val_acc: 0.9355 - val_f1_m: 0.9227\n",
      "\n",
      "Epoch 00070: val_f1_m did not improve from 0.95620\n",
      "Epoch 71/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.3672 - acc: 0.9961 - f1_m: 0.9922 - val_loss: 0.3662 - val_acc: 0.9355 - val_f1_m: 0.9312\n",
      "\n",
      "Epoch 00071: val_f1_m did not improve from 0.95620\n",
      "Epoch 72/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.3705 - acc: 0.9960 - f1_m: 0.9933 - val_loss: 0.2749 - val_acc: 0.9442 - val_f1_m: 0.9369\n",
      "\n",
      "Epoch 00072: val_f1_m did not improve from 0.95620\n",
      "Epoch 73/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.3657 - acc: 0.9966 - f1_m: 0.9934 - val_loss: 0.3052 - val_acc: 0.9375 - val_f1_m: 0.9293\n",
      "\n",
      "Epoch 00073: val_f1_m did not improve from 0.95620\n",
      "\n",
      "Epoch 00073: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "Epoch 74/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.3632 - acc: 0.9961 - f1_m: 0.9927 - val_loss: 0.2839 - val_acc: 0.9473 - val_f1_m: 0.9379\n",
      "\n",
      "Epoch 00074: val_f1_m did not improve from 0.95620\n",
      "Epoch 75/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.3653 - acc: 0.9959 - f1_m: 0.9924 - val_loss: 0.2947 - val_acc: 0.9395 - val_f1_m: 0.9342\n",
      "\n",
      "Epoch 00075: val_f1_m did not improve from 0.95620\n",
      "Epoch 76/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.3646 - acc: 0.9969 - f1_m: 0.9932 - val_loss: 0.3327 - val_acc: 0.9382 - val_f1_m: 0.9305\n",
      "\n",
      "Epoch 00076: val_f1_m did not improve from 0.95620\n",
      "Epoch 77/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.3684 - acc: 0.9948 - f1_m: 0.9915 - val_loss: 0.3137 - val_acc: 0.9414 - val_f1_m: 0.9346\n",
      "\n",
      "Epoch 00077: val_f1_m did not improve from 0.95620\n",
      "Epoch 78/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.3605 - acc: 0.9964 - f1_m: 0.9927 - val_loss: 0.2652 - val_acc: 0.9434 - val_f1_m: 0.9385\n",
      "\n",
      "Epoch 00078: val_f1_m did not improve from 0.95620\n",
      "Epoch 79/150\n",
      "497/497 [==============================] - 226s 454ms/step - loss: 0.3625 - acc: 0.9970 - f1_m: 0.9939 - val_loss: 0.2862 - val_acc: 0.9492 - val_f1_m: 0.9334\n",
      "\n",
      "Epoch 00079: val_f1_m did not improve from 0.95620\n",
      "Epoch 80/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.3653 - acc: 0.9950 - f1_m: 0.9915 - val_loss: 0.3527 - val_acc: 0.9363 - val_f1_m: 0.9240\n",
      "\n",
      "Epoch 00080: val_f1_m did not improve from 0.95620\n",
      "Epoch 81/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.3647 - acc: 0.9957 - f1_m: 0.9924 - val_loss: 0.3854 - val_acc: 0.9199 - val_f1_m: 0.9137\n",
      "\n",
      "Epoch 00081: val_f1_m did not improve from 0.95620\n",
      "Epoch 82/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.3606 - acc: 0.9959 - f1_m: 0.9926 - val_loss: 0.2739 - val_acc: 0.9375 - val_f1_m: 0.9370\n",
      "\n",
      "Epoch 00082: val_f1_m did not improve from 0.95620\n",
      "Epoch 83/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.3624 - acc: 0.9970 - f1_m: 0.9928 - val_loss: 0.2852 - val_acc: 0.9434 - val_f1_m: 0.9275\n",
      "\n",
      "Epoch 00083: val_f1_m did not improve from 0.95620\n",
      "Epoch 84/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.3616 - acc: 0.9960 - f1_m: 0.9937 - val_loss: 0.2740 - val_acc: 0.9542 - val_f1_m: 0.9495\n",
      "\n",
      "Epoch 00084: val_f1_m did not improve from 0.95620\n",
      "Epoch 85/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.3652 - acc: 0.9961 - f1_m: 0.9933 - val_loss: 0.2440 - val_acc: 0.9434 - val_f1_m: 0.9394\n",
      "\n",
      "Epoch 00085: val_f1_m did not improve from 0.95620\n",
      "Epoch 86/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.3646 - acc: 0.9960 - f1_m: 0.9930 - val_loss: 0.2868 - val_acc: 0.9414 - val_f1_m: 0.9352\n",
      "\n",
      "Epoch 00086: val_f1_m did not improve from 0.95620\n",
      "Epoch 87/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.3602 - acc: 0.9960 - f1_m: 0.9936 - val_loss: 0.3103 - val_acc: 0.9453 - val_f1_m: 0.9337\n",
      "\n",
      "Epoch 00087: val_f1_m did not improve from 0.95620\n",
      "Epoch 88/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.3647 - acc: 0.9961 - f1_m: 0.9931 - val_loss: 0.3872 - val_acc: 0.9263 - val_f1_m: 0.9118\n",
      "\n",
      "Epoch 00088: val_f1_m did not improve from 0.95620\n",
      "Epoch 89/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.3615 - acc: 0.9959 - f1_m: 0.9932 - val_loss: 0.3649 - val_acc: 0.9375 - val_f1_m: 0.9260\n",
      "\n",
      "Epoch 00089: val_f1_m did not improve from 0.95620\n",
      "Epoch 90/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.3628 - acc: 0.9962 - f1_m: 0.9928 - val_loss: 0.2743 - val_acc: 0.9395 - val_f1_m: 0.9378\n",
      "\n",
      "Epoch 00090: val_f1_m did not improve from 0.95620\n",
      "Epoch 91/150\n",
      "497/497 [==============================] - 226s 455ms/step - loss: 0.3593 - acc: 0.9972 - f1_m: 0.9942 - val_loss: 0.2839 - val_acc: 0.9375 - val_f1_m: 0.9355\n",
      "\n",
      "Epoch 00091: val_f1_m did not improve from 0.95620\n",
      "Epoch 00091: early stopping\n",
      "[*]-------------------EfficientNetB3 model_foldstep_2-------------------\n",
      ">>image_size: 299\n",
      ">>train_path: ../input/train_crop_299\n",
      ">>test_path: ../input/test_crop_299\n",
      "Found 7985 validated image filenames belonging to 196 classes.\n",
      "Found 7985 validated image filenames belonging to 196 classes.\n",
      "Found 2005 validated image filenames belonging to 196 classes.\n",
      "Found 6150 validated image filenames.\n",
      ">>model path to save: ../model/emsemble_final_EfficientNetB3_fold_3.hdf5\n",
      "Epoch 1/150\n",
      "500/500 [==============================] - 245s 491ms/step - loss: 5.2152 - acc: 0.0159 - f1_m: 0.0000e+00 - val_loss: 4.6622 - val_acc: 0.0801 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_f1_m improved from -inf to 0.00000, saving model to ../model/emsemble_final_EfficientNetB3_fold_3.hdf5\n",
      "Epoch 2/150\n",
      "500/500 [==============================] - 228s 456ms/step - loss: 4.3228 - acc: 0.1165 - f1_m: 0.0096 - val_loss: 2.9545 - val_acc: 0.2793 - val_f1_m: 0.0922\n",
      "\n",
      "Epoch 00002: val_f1_m improved from 0.00000 to 0.09216, saving model to ../model/emsemble_final_EfficientNetB3_fold_3.hdf5\n",
      "Epoch 3/150\n",
      "500/500 [==============================] - 228s 457ms/step - loss: 3.1517 - acc: 0.3071 - f1_m: 0.1021 - val_loss: 1.6760 - val_acc: 0.5957 - val_f1_m: 0.4146\n",
      "\n",
      "Epoch 00003: val_f1_m improved from 0.09216 to 0.41456, saving model to ../model/emsemble_final_EfficientNetB3_fold_3.hdf5\n",
      "Epoch 4/150\n",
      "500/500 [==============================] - 228s 457ms/step - loss: 2.3521 - acc: 0.4721 - f1_m: 0.3176 - val_loss: 1.3069 - val_acc: 0.6367 - val_f1_m: 0.5891\n",
      "\n",
      "Epoch 00004: val_f1_m improved from 0.41456 to 0.58909, saving model to ../model/emsemble_final_EfficientNetB3_fold_3.hdf5\n",
      "Epoch 5/150\n",
      "500/500 [==============================] - 228s 457ms/step - loss: 1.8565 - acc: 0.5874 - f1_m: 0.4929 - val_loss: 0.8322 - val_acc: 0.7871 - val_f1_m: 0.7708\n",
      "\n",
      "Epoch 00005: val_f1_m improved from 0.58909 to 0.77077, saving model to ../model/emsemble_final_EfficientNetB3_fold_3.hdf5\n",
      "Epoch 6/150\n",
      "500/500 [==============================] - 228s 456ms/step - loss: 1.5480 - acc: 0.6664 - f1_m: 0.6038 - val_loss: 0.6607 - val_acc: 0.7988 - val_f1_m: 0.7832\n",
      "\n",
      "Epoch 00006: val_f1_m improved from 0.77077 to 0.78321, saving model to ../model/emsemble_final_EfficientNetB3_fold_3.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/150\n",
      "500/500 [==============================] - 228s 457ms/step - loss: 1.3221 - acc: 0.7268 - f1_m: 0.6829 - val_loss: 0.5666 - val_acc: 0.8340 - val_f1_m: 0.8207\n",
      "\n",
      "Epoch 00007: val_f1_m improved from 0.78321 to 0.82075, saving model to ../model/emsemble_final_EfficientNetB3_fold_3.hdf5\n",
      "Epoch 8/150\n",
      "500/500 [==============================] - 228s 456ms/step - loss: 1.1599 - acc: 0.7755 - f1_m: 0.7401 - val_loss: 0.4997 - val_acc: 0.8583 - val_f1_m: 0.8467\n",
      "\n",
      "Epoch 00008: val_f1_m improved from 0.82075 to 0.84672, saving model to ../model/emsemble_final_EfficientNetB3_fold_3.hdf5\n",
      "Epoch 9/150\n",
      "500/500 [==============================] - 228s 456ms/step - loss: 1.0415 - acc: 0.8035 - f1_m: 0.7839 - val_loss: 0.4295 - val_acc: 0.8711 - val_f1_m: 0.8665\n",
      "\n",
      "Epoch 00009: val_f1_m improved from 0.84672 to 0.86652, saving model to ../model/emsemble_final_EfficientNetB3_fold_3.hdf5\n",
      "Epoch 10/150\n",
      "500/500 [==============================] - 228s 456ms/step - loss: 0.9420 - acc: 0.8396 - f1_m: 0.8168 - val_loss: 0.4916 - val_acc: 0.8730 - val_f1_m: 0.8702\n",
      "\n",
      "Epoch 00010: val_f1_m improved from 0.86652 to 0.87022, saving model to ../model/emsemble_final_EfficientNetB3_fold_3.hdf5\n",
      "Epoch 11/150\n",
      "500/500 [==============================] - 228s 457ms/step - loss: 0.8634 - acc: 0.8538 - f1_m: 0.8325 - val_loss: 0.4447 - val_acc: 0.8691 - val_f1_m: 0.8738\n",
      "\n",
      "Epoch 00011: val_f1_m improved from 0.87022 to 0.87383, saving model to ../model/emsemble_final_EfficientNetB3_fold_3.hdf5\n",
      "Epoch 12/150\n",
      "500/500 [==============================] - 228s 456ms/step - loss: 0.8254 - acc: 0.8750 - f1_m: 0.8626 - val_loss: 0.4251 - val_acc: 0.8643 - val_f1_m: 0.8635\n",
      "\n",
      "Epoch 00012: val_f1_m did not improve from 0.87383\n",
      "Epoch 13/150\n",
      "500/500 [==============================] - 227s 454ms/step - loss: 0.7690 - acc: 0.8908 - f1_m: 0.8764 - val_loss: 0.3868 - val_acc: 0.8887 - val_f1_m: 0.8899\n",
      "\n",
      "Epoch 00013: val_f1_m improved from 0.87383 to 0.88990, saving model to ../model/emsemble_final_EfficientNetB3_fold_3.hdf5\n",
      "Epoch 14/150\n",
      "500/500 [==============================] - 228s 457ms/step - loss: 0.7326 - acc: 0.9005 - f1_m: 0.8862 - val_loss: 0.3536 - val_acc: 0.9023 - val_f1_m: 0.8998\n",
      "\n",
      "Epoch 00014: val_f1_m improved from 0.88990 to 0.89984, saving model to ../model/emsemble_final_EfficientNetB3_fold_3.hdf5\n",
      "Epoch 15/150\n",
      "500/500 [==============================] - 228s 457ms/step - loss: 0.6997 - acc: 0.9119 - f1_m: 0.8969 - val_loss: 0.3057 - val_acc: 0.9004 - val_f1_m: 0.9105\n",
      "\n",
      "Epoch 00015: val_f1_m improved from 0.89984 to 0.91048, saving model to ../model/emsemble_final_EfficientNetB3_fold_3.hdf5\n",
      "Epoch 16/150\n",
      "500/500 [==============================] - 228s 457ms/step - loss: 0.6682 - acc: 0.9239 - f1_m: 0.9124 - val_loss: 0.4356 - val_acc: 0.8882 - val_f1_m: 0.8864\n",
      "\n",
      "Epoch 00016: val_f1_m did not improve from 0.91048\n",
      "Epoch 17/150\n",
      "500/500 [==============================] - 228s 457ms/step - loss: 0.6510 - acc: 0.9275 - f1_m: 0.9137 - val_loss: 0.3046 - val_acc: 0.9121 - val_f1_m: 0.9132\n",
      "\n",
      "Epoch 00017: val_f1_m improved from 0.91048 to 0.91318, saving model to ../model/emsemble_final_EfficientNetB3_fold_3.hdf5\n",
      "Epoch 18/150\n",
      "500/500 [==============================] - 228s 457ms/step - loss: 0.6273 - acc: 0.9359 - f1_m: 0.9230 - val_loss: 0.3099 - val_acc: 0.9141 - val_f1_m: 0.9129\n",
      "\n",
      "Epoch 00018: val_f1_m did not improve from 0.91318\n",
      "Epoch 19/150\n",
      "500/500 [==============================] - 228s 457ms/step - loss: 0.5971 - acc: 0.9445 - f1_m: 0.9311 - val_loss: 0.3811 - val_acc: 0.9102 - val_f1_m: 0.9131\n",
      "\n",
      "Epoch 00019: val_f1_m did not improve from 0.91318\n",
      "Epoch 20/150\n",
      "500/500 [==============================] - 228s 456ms/step - loss: 0.5922 - acc: 0.9456 - f1_m: 0.9358 - val_loss: 0.3776 - val_acc: 0.8922 - val_f1_m: 0.8998\n",
      "\n",
      "Epoch 00020: val_f1_m did not improve from 0.91318\n",
      "Epoch 21/150\n",
      "500/500 [==============================] - 227s 455ms/step - loss: 0.5678 - acc: 0.9555 - f1_m: 0.9420 - val_loss: 0.3535 - val_acc: 0.9004 - val_f1_m: 0.9036\n",
      "\n",
      "Epoch 00021: val_f1_m did not improve from 0.91318\n",
      "Epoch 22/150\n",
      "500/500 [==============================] - 228s 457ms/step - loss: 0.5595 - acc: 0.9531 - f1_m: 0.9428 - val_loss: 0.3518 - val_acc: 0.9121 - val_f1_m: 0.9134\n",
      "\n",
      "Epoch 00022: val_f1_m improved from 0.91318 to 0.91336, saving model to ../model/emsemble_final_EfficientNetB3_fold_3.hdf5\n",
      "Epoch 23/150\n",
      "500/500 [==============================] - 228s 457ms/step - loss: 0.5497 - acc: 0.9578 - f1_m: 0.9478 - val_loss: 0.3589 - val_acc: 0.9199 - val_f1_m: 0.9138\n",
      "\n",
      "Epoch 00023: val_f1_m improved from 0.91336 to 0.91379, saving model to ../model/emsemble_final_EfficientNetB3_fold_3.hdf5\n",
      "Epoch 24/150\n",
      "500/500 [==============================] - 228s 456ms/step - loss: 0.5337 - acc: 0.9618 - f1_m: 0.9525 - val_loss: 0.3693 - val_acc: 0.9022 - val_f1_m: 0.9028\n",
      "\n",
      "Epoch 00024: val_f1_m did not improve from 0.91379\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Epoch 25/150\n",
      "500/500 [==============================] - 228s 457ms/step - loss: 0.5108 - acc: 0.9684 - f1_m: 0.9605 - val_loss: 0.3649 - val_acc: 0.9160 - val_f1_m: 0.9153\n",
      "\n",
      "Epoch 00025: val_f1_m improved from 0.91379 to 0.91533, saving model to ../model/emsemble_final_EfficientNetB3_fold_3.hdf5\n",
      "Epoch 26/150\n",
      "500/500 [==============================] - 227s 454ms/step - loss: 0.4963 - acc: 0.9720 - f1_m: 0.9613 - val_loss: 0.3301 - val_acc: 0.9062 - val_f1_m: 0.9055\n",
      "\n",
      "Epoch 00026: val_f1_m did not improve from 0.91533\n",
      "Epoch 27/150\n",
      "500/500 [==============================] - 227s 455ms/step - loss: 0.4905 - acc: 0.9735 - f1_m: 0.9662 - val_loss: 0.3470 - val_acc: 0.9238 - val_f1_m: 0.9217\n",
      "\n",
      "Epoch 00027: val_f1_m improved from 0.91533 to 0.92165, saving model to ../model/emsemble_final_EfficientNetB3_fold_3.hdf5\n",
      "Epoch 28/150\n",
      "500/500 [==============================] - 227s 454ms/step - loss: 0.4856 - acc: 0.9748 - f1_m: 0.9675 - val_loss: 0.2870 - val_acc: 0.9301 - val_f1_m: 0.9341\n",
      "\n",
      "Epoch 00028: val_f1_m improved from 0.92165 to 0.93414, saving model to ../model/emsemble_final_EfficientNetB3_fold_3.hdf5\n",
      "Epoch 29/150\n",
      "500/500 [==============================] - 228s 457ms/step - loss: 0.4742 - acc: 0.9766 - f1_m: 0.9698 - val_loss: 0.2547 - val_acc: 0.9355 - val_f1_m: 0.9302\n",
      "\n",
      "Epoch 00029: val_f1_m did not improve from 0.93414\n",
      "Epoch 30/150\n",
      "500/500 [==============================] - 229s 457ms/step - loss: 0.4655 - acc: 0.9803 - f1_m: 0.9717 - val_loss: 0.3038 - val_acc: 0.9258 - val_f1_m: 0.9228\n",
      "\n",
      "Epoch 00030: val_f1_m did not improve from 0.93414\n",
      "Epoch 31/150\n",
      "500/500 [==============================] - 228s 457ms/step - loss: 0.4581 - acc: 0.9812 - f1_m: 0.9713 - val_loss: 0.4232 - val_acc: 0.8945 - val_f1_m: 0.8976\n",
      "\n",
      "Epoch 00031: val_f1_m did not improve from 0.93414\n",
      "Epoch 32/150\n",
      "500/500 [==============================] - 228s 456ms/step - loss: 0.4634 - acc: 0.9790 - f1_m: 0.9734 - val_loss: 0.3798 - val_acc: 0.9002 - val_f1_m: 0.9025\n",
      "\n",
      "Epoch 00032: val_f1_m did not improve from 0.93414\n",
      "Epoch 33/150\n",
      "500/500 [==============================] - 228s 457ms/step - loss: 0.4506 - acc: 0.9821 - f1_m: 0.9728 - val_loss: 0.2887 - val_acc: 0.9316 - val_f1_m: 0.9293\n",
      "\n",
      "Epoch 00033: val_f1_m did not improve from 0.93414\n",
      "Epoch 34/150\n",
      "500/500 [==============================] - 228s 457ms/step - loss: 0.4511 - acc: 0.9801 - f1_m: 0.9737 - val_loss: 0.4032 - val_acc: 0.9102 - val_f1_m: 0.9045\n",
      "\n",
      "Epoch 00034: val_f1_m did not improve from 0.93414\n",
      "Epoch 35/150\n",
      "500/500 [==============================] - 228s 456ms/step - loss: 0.4461 - acc: 0.9835 - f1_m: 0.9758 - val_loss: 0.2708 - val_acc: 0.9414 - val_f1_m: 0.9354\n",
      "\n",
      "Epoch 00035: val_f1_m improved from 0.93414 to 0.93541, saving model to ../model/emsemble_final_EfficientNetB3_fold_3.hdf5\n",
      "Epoch 36/150\n",
      "500/500 [==============================] - 228s 457ms/step - loss: 0.4379 - acc: 0.9851 - f1_m: 0.9756 - val_loss: 0.2868 - val_acc: 0.9381 - val_f1_m: 0.9365\n",
      "\n",
      "Epoch 00036: val_f1_m improved from 0.93541 to 0.93655, saving model to ../model/emsemble_final_EfficientNetB3_fold_3.hdf5\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 37/150\n",
      "500/500 [==============================] - 227s 454ms/step - loss: 0.4341 - acc: 0.9846 - f1_m: 0.9796 - val_loss: 0.3634 - val_acc: 0.9062 - val_f1_m: 0.9092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00037: val_f1_m did not improve from 0.93655\n",
      "Epoch 38/150\n",
      "500/500 [==============================] - 227s 454ms/step - loss: 0.4360 - acc: 0.9850 - f1_m: 0.9790 - val_loss: 0.3172 - val_acc: 0.9258 - val_f1_m: 0.9177\n",
      "\n",
      "Epoch 00038: val_f1_m did not improve from 0.93655\n",
      "Epoch 39/150\n",
      "500/500 [==============================] - 227s 455ms/step - loss: 0.4335 - acc: 0.9834 - f1_m: 0.9783 - val_loss: 0.3516 - val_acc: 0.9258 - val_f1_m: 0.9147\n",
      "\n",
      "Epoch 00039: val_f1_m did not improve from 0.93655\n",
      "Epoch 40/150\n",
      "500/500 [==============================] - 227s 455ms/step - loss: 0.4318 - acc: 0.9841 - f1_m: 0.9792 - val_loss: 0.2927 - val_acc: 0.9261 - val_f1_m: 0.9238\n",
      "\n",
      "Epoch 00040: val_f1_m did not improve from 0.93655\n",
      "Epoch 41/150\n",
      "500/500 [==============================] - 227s 454ms/step - loss: 0.4205 - acc: 0.9881 - f1_m: 0.9834 - val_loss: 0.3124 - val_acc: 0.9277 - val_f1_m: 0.9245\n",
      "\n",
      "Epoch 00041: val_f1_m did not improve from 0.93655\n",
      "Epoch 42/150\n",
      "500/500 [==============================] - 227s 455ms/step - loss: 0.4251 - acc: 0.9860 - f1_m: 0.9813 - val_loss: 0.3924 - val_acc: 0.9160 - val_f1_m: 0.9117\n",
      "\n",
      "Epoch 00042: val_f1_m did not improve from 0.93655\n",
      "Epoch 43/150\n",
      "500/500 [==============================] - 227s 455ms/step - loss: 0.4141 - acc: 0.9904 - f1_m: 0.9829 - val_loss: 0.2915 - val_acc: 0.9355 - val_f1_m: 0.9303\n",
      "\n",
      "Epoch 00043: val_f1_m did not improve from 0.93655\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 44/150\n",
      "500/500 [==============================] - 227s 454ms/step - loss: 0.4200 - acc: 0.9873 - f1_m: 0.9820 - val_loss: 0.2874 - val_acc: 0.9281 - val_f1_m: 0.9304\n",
      "\n",
      "Epoch 00044: val_f1_m did not improve from 0.93655\n",
      "Epoch 45/150\n",
      "500/500 [==============================] - 227s 454ms/step - loss: 0.4168 - acc: 0.9876 - f1_m: 0.9817 - val_loss: 0.3426 - val_acc: 0.9219 - val_f1_m: 0.9148\n",
      "\n",
      "Epoch 00045: val_f1_m did not improve from 0.93655\n",
      "Epoch 46/150\n",
      "500/500 [==============================] - 228s 455ms/step - loss: 0.4107 - acc: 0.9886 - f1_m: 0.9836 - val_loss: 0.3435 - val_acc: 0.9238 - val_f1_m: 0.9154\n",
      "\n",
      "Epoch 00046: val_f1_m did not improve from 0.93655\n",
      "Epoch 47/150\n",
      "500/500 [==============================] - 227s 455ms/step - loss: 0.4146 - acc: 0.9883 - f1_m: 0.9847 - val_loss: 0.3163 - val_acc: 0.9297 - val_f1_m: 0.9241\n",
      "\n",
      "Epoch 00047: val_f1_m did not improve from 0.93655\n",
      "Epoch 48/150\n",
      "500/500 [==============================] - 228s 457ms/step - loss: 0.4086 - acc: 0.9880 - f1_m: 0.9832 - val_loss: 0.3379 - val_acc: 0.9261 - val_f1_m: 0.9196\n",
      "\n",
      "Epoch 00048: val_f1_m did not improve from 0.93655\n",
      "Epoch 49/150\n",
      "500/500 [==============================] - 228s 457ms/step - loss: 0.4064 - acc: 0.9918 - f1_m: 0.9865 - val_loss: 0.2850 - val_acc: 0.9375 - val_f1_m: 0.9244\n",
      "\n",
      "Epoch 00049: val_f1_m did not improve from 0.93655\n",
      "Epoch 50/150\n",
      "500/500 [==============================] - 228s 456ms/step - loss: 0.4076 - acc: 0.9904 - f1_m: 0.9840 - val_loss: 0.2574 - val_acc: 0.9395 - val_f1_m: 0.9458\n",
      "\n",
      "Epoch 00050: val_f1_m improved from 0.93655 to 0.94579, saving model to ../model/emsemble_final_EfficientNetB3_fold_3.hdf5\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "Epoch 51/150\n",
      "500/500 [==============================] - 228s 457ms/step - loss: 0.4135 - acc: 0.9891 - f1_m: 0.9842 - val_loss: 0.4310 - val_acc: 0.9004 - val_f1_m: 0.8994\n",
      "\n",
      "Epoch 00051: val_f1_m did not improve from 0.94579\n",
      "Epoch 52/150\n",
      "500/500 [==============================] - 227s 454ms/step - loss: 0.4072 - acc: 0.9904 - f1_m: 0.9839 - val_loss: 0.2515 - val_acc: 0.9421 - val_f1_m: 0.9451\n",
      "\n",
      "Epoch 00052: val_f1_m did not improve from 0.94579\n",
      "Epoch 53/150\n",
      "500/500 [==============================] - 228s 456ms/step - loss: 0.4040 - acc: 0.9893 - f1_m: 0.9845 - val_loss: 0.3354 - val_acc: 0.9258 - val_f1_m: 0.9233\n",
      "\n",
      "Epoch 00053: val_f1_m did not improve from 0.94579\n",
      "Epoch 54/150\n",
      "500/500 [==============================] - 229s 457ms/step - loss: 0.4074 - acc: 0.9895 - f1_m: 0.9841 - val_loss: 0.3322 - val_acc: 0.9238 - val_f1_m: 0.9133\n",
      "\n",
      "Epoch 00054: val_f1_m did not improve from 0.94579\n",
      "Epoch 55/150\n",
      "500/500 [==============================] - 228s 456ms/step - loss: 0.4081 - acc: 0.9900 - f1_m: 0.9849 - val_loss: 0.3379 - val_acc: 0.9277 - val_f1_m: 0.9258\n",
      "\n",
      "Epoch 00055: val_f1_m did not improve from 0.94579\n",
      "Epoch 56/150\n",
      "500/500 [==============================] - 227s 455ms/step - loss: 0.4119 - acc: 0.9885 - f1_m: 0.9835 - val_loss: 0.3517 - val_acc: 0.9261 - val_f1_m: 0.9176\n",
      "\n",
      "Epoch 00056: val_f1_m did not improve from 0.94579\n",
      "Epoch 57/150\n",
      "500/500 [==============================] - 228s 457ms/step - loss: 0.4055 - acc: 0.9899 - f1_m: 0.9858 - val_loss: 0.3135 - val_acc: 0.9453 - val_f1_m: 0.9305\n",
      "\n",
      "Epoch 00057: val_f1_m did not improve from 0.94579\n",
      "Epoch 58/150\n",
      "500/500 [==============================] - 228s 456ms/step - loss: 0.4068 - acc: 0.9910 - f1_m: 0.9845 - val_loss: 0.3470 - val_acc: 0.9219 - val_f1_m: 0.9104\n",
      "\n",
      "Epoch 00058: val_f1_m did not improve from 0.94579\n",
      "Epoch 59/150\n",
      "500/500 [==============================] - 228s 456ms/step - loss: 0.3999 - acc: 0.9906 - f1_m: 0.9858 - val_loss: 0.2963 - val_acc: 0.9297 - val_f1_m: 0.9350\n",
      "\n",
      "Epoch 00059: val_f1_m did not improve from 0.94579\n",
      "Epoch 60/150\n",
      "500/500 [==============================] - 228s 456ms/step - loss: 0.4075 - acc: 0.9899 - f1_m: 0.9848 - val_loss: 0.3050 - val_acc: 0.9361 - val_f1_m: 0.9280\n",
      "\n",
      "Epoch 00060: val_f1_m did not improve from 0.94579\n",
      "Epoch 61/150\n",
      "500/500 [==============================] - 228s 457ms/step - loss: 0.4100 - acc: 0.9900 - f1_m: 0.9851 - val_loss: 0.3190 - val_acc: 0.9316 - val_f1_m: 0.9226\n",
      "\n",
      "Epoch 00061: val_f1_m did not improve from 0.94579\n",
      "Epoch 62/150\n",
      "500/500 [==============================] - 228s 457ms/step - loss: 0.4025 - acc: 0.9899 - f1_m: 0.9869 - val_loss: 0.2597 - val_acc: 0.9453 - val_f1_m: 0.9372\n",
      "\n",
      "Epoch 00062: val_f1_m did not improve from 0.94579\n",
      "Epoch 63/150\n",
      "500/500 [==============================] - 228s 457ms/step - loss: 0.3965 - acc: 0.9917 - f1_m: 0.9863 - val_loss: 0.3919 - val_acc: 0.9082 - val_f1_m: 0.9088\n",
      "\n",
      "Epoch 00063: val_f1_m did not improve from 0.94579\n",
      "Epoch 64/150\n",
      "500/500 [==============================] - 227s 455ms/step - loss: 0.3984 - acc: 0.9884 - f1_m: 0.9860 - val_loss: 0.3037 - val_acc: 0.9414 - val_f1_m: 0.9171\n",
      "\n",
      "Epoch 00064: val_f1_m did not improve from 0.94579\n",
      "Epoch 65/150\n",
      "500/500 [==============================] - 228s 457ms/step - loss: 0.3976 - acc: 0.9921 - f1_m: 0.9850 - val_loss: 0.3161 - val_acc: 0.9238 - val_f1_m: 0.9234\n",
      "\n",
      "Epoch 00065: val_f1_m did not improve from 0.94579\n",
      "Epoch 66/150\n",
      "500/500 [==============================] - 228s 456ms/step - loss: 0.4012 - acc: 0.9901 - f1_m: 0.9854 - val_loss: 0.3257 - val_acc: 0.9375 - val_f1_m: 0.9333\n",
      "\n",
      "Epoch 00066: val_f1_m did not improve from 0.94579\n",
      "Epoch 67/150\n",
      "500/500 [==============================] - 228s 456ms/step - loss: 0.4042 - acc: 0.9898 - f1_m: 0.9848 - val_loss: 0.3474 - val_acc: 0.9182 - val_f1_m: 0.9135\n",
      "\n",
      "Epoch 00067: val_f1_m did not improve from 0.94579\n",
      "Epoch 68/150\n",
      "500/500 [==============================] - 228s 457ms/step - loss: 0.3987 - acc: 0.9905 - f1_m: 0.9862 - val_loss: 0.3083 - val_acc: 0.9297 - val_f1_m: 0.9173\n",
      "\n",
      "Epoch 00068: val_f1_m did not improve from 0.94579\n",
      "Epoch 69/150\n",
      "500/500 [==============================] - 228s 457ms/step - loss: 0.4010 - acc: 0.9936 - f1_m: 0.9877 - val_loss: 0.3220 - val_acc: 0.9414 - val_f1_m: 0.9301\n",
      "\n",
      "Epoch 00069: val_f1_m did not improve from 0.94579\n",
      "Epoch 70/150\n",
      "500/500 [==============================] - 227s 455ms/step - loss: 0.3962 - acc: 0.9931 - f1_m: 0.9876 - val_loss: 0.3326 - val_acc: 0.9238 - val_f1_m: 0.9254\n",
      "\n",
      "Epoch 00070: val_f1_m did not improve from 0.94579\n",
      "Epoch 71/150\n",
      "500/500 [==============================] - 228s 457ms/step - loss: 0.4023 - acc: 0.9898 - f1_m: 0.9858 - val_loss: 0.3383 - val_acc: 0.9202 - val_f1_m: 0.9102\n",
      "\n",
      "Epoch 00071: val_f1_m did not improve from 0.94579\n",
      "Epoch 72/150\n",
      "500/500 [==============================] - 228s 457ms/step - loss: 0.3984 - acc: 0.9916 - f1_m: 0.9869 - val_loss: 0.3100 - val_acc: 0.9297 - val_f1_m: 0.9213\n",
      "\n",
      "Epoch 00072: val_f1_m did not improve from 0.94579\n",
      "Epoch 73/150\n",
      "500/500 [==============================] - 228s 457ms/step - loss: 0.3997 - acc: 0.9905 - f1_m: 0.9852 - val_loss: 0.3028 - val_acc: 0.9414 - val_f1_m: 0.9348\n",
      "\n",
      "Epoch 00073: val_f1_m did not improve from 0.94579\n",
      "Epoch 74/150\n",
      "500/500 [==============================] - 228s 457ms/step - loss: 0.3958 - acc: 0.9916 - f1_m: 0.9877 - val_loss: 0.3140 - val_acc: 0.9414 - val_f1_m: 0.9272\n",
      "\n",
      "Epoch 00074: val_f1_m did not improve from 0.94579\n",
      "Epoch 75/150\n",
      "500/500 [==============================] - 228s 456ms/step - loss: 0.4006 - acc: 0.9901 - f1_m: 0.9860 - val_loss: 0.3546 - val_acc: 0.9182 - val_f1_m: 0.9115\n",
      "\n",
      "Epoch 00075: val_f1_m did not improve from 0.94579\n",
      "Epoch 00075: early stopping\n",
      "[*]-------------------EfficientNetB3 model_foldstep_3-------------------\n",
      ">>image_size: 299\n",
      ">>train_path: ../input/train_crop_299\n",
      ">>test_path: ../input/test_crop_299\n",
      "Found 8031 validated image filenames belonging to 196 classes.\n",
      "Found 8031 validated image filenames belonging to 196 classes.\n",
      "Found 1959 validated image filenames belonging to 196 classes.\n",
      "Found 6150 validated image filenames.\n",
      ">>model path to save: ../model/emsemble_final_EfficientNetB3_fold_4.hdf5\n",
      "Epoch 1/150\n",
      "502/502 [==============================] - 256s 509ms/step - loss: 5.2005 - acc: 0.0169 - f1_m: 0.0000e+00 - val_loss: 4.6390 - val_acc: 0.0847 - val_f1_m: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_f1_m improved from -inf to 0.00000, saving model to ../model/emsemble_final_EfficientNetB3_fold_4.hdf5\n",
      "Epoch 2/150\n",
      "502/502 [==============================] - 233s 464ms/step - loss: 4.1832 - acc: 0.1315 - f1_m: 0.0168 - val_loss: 2.9056 - val_acc: 0.3427 - val_f1_m: 0.1225\n",
      "\n",
      "Epoch 00002: val_f1_m improved from 0.00000 to 0.12255, saving model to ../model/emsemble_final_EfficientNetB3_fold_4.hdf5\n",
      "Epoch 3/150\n",
      "502/502 [==============================] - 233s 464ms/step - loss: 2.9799 - acc: 0.3334 - f1_m: 0.1303 - val_loss: 1.7066 - val_acc: 0.5867 - val_f1_m: 0.4770\n",
      "\n",
      "Epoch 00003: val_f1_m improved from 0.12255 to 0.47701, saving model to ../model/emsemble_final_EfficientNetB3_fold_4.hdf5\n",
      "Epoch 4/150\n",
      "502/502 [==============================] - 233s 464ms/step - loss: 2.2223 - acc: 0.4946 - f1_m: 0.3577 - val_loss: 1.1812 - val_acc: 0.6694 - val_f1_m: 0.6117\n",
      "\n",
      "Epoch 00004: val_f1_m improved from 0.47701 to 0.61169, saving model to ../model/emsemble_final_EfficientNetB3_fold_4.hdf5\n",
      "Epoch 5/150\n",
      "502/502 [==============================] - 232s 461ms/step - loss: 1.7620 - acc: 0.6161 - f1_m: 0.5271 - val_loss: 0.9421 - val_acc: 0.7198 - val_f1_m: 0.7127\n",
      "\n",
      "Epoch 00005: val_f1_m improved from 0.61169 to 0.71266, saving model to ../model/emsemble_final_EfficientNetB3_fold_4.hdf5\n",
      "Epoch 6/150\n",
      "502/502 [==============================] - 232s 462ms/step - loss: 1.4564 - acc: 0.6875 - f1_m: 0.6324 - val_loss: 0.7247 - val_acc: 0.7782 - val_f1_m: 0.7728\n",
      "\n",
      "Epoch 00006: val_f1_m improved from 0.71266 to 0.77279, saving model to ../model/emsemble_final_EfficientNetB3_fold_4.hdf5\n",
      "Epoch 7/150\n",
      "502/502 [==============================] - 232s 462ms/step - loss: 1.2753 - acc: 0.7334 - f1_m: 0.7004 - val_loss: 0.7185 - val_acc: 0.8125 - val_f1_m: 0.7884\n",
      "\n",
      "Epoch 00007: val_f1_m improved from 0.77279 to 0.78841, saving model to ../model/emsemble_final_EfficientNetB3_fold_4.hdf5\n",
      "Epoch 8/150\n",
      "502/502 [==============================] - 232s 462ms/step - loss: 1.1178 - acc: 0.7797 - f1_m: 0.7515 - val_loss: 0.5464 - val_acc: 0.8337 - val_f1_m: 0.8423\n",
      "\n",
      "Epoch 00008: val_f1_m improved from 0.78841 to 0.84232, saving model to ../model/emsemble_final_EfficientNetB3_fold_4.hdf5\n",
      "Epoch 9/150\n",
      "502/502 [==============================] - 233s 464ms/step - loss: 0.9941 - acc: 0.8227 - f1_m: 0.7922 - val_loss: 0.4958 - val_acc: 0.8569 - val_f1_m: 0.8469\n",
      "\n",
      "Epoch 00009: val_f1_m improved from 0.84232 to 0.84689, saving model to ../model/emsemble_final_EfficientNetB3_fold_4.hdf5\n",
      "Epoch 10/150\n",
      "502/502 [==============================] - 233s 463ms/step - loss: 0.9088 - acc: 0.8463 - f1_m: 0.8255 - val_loss: 0.4655 - val_acc: 0.8609 - val_f1_m: 0.8528\n",
      "\n",
      "Epoch 00010: val_f1_m improved from 0.84689 to 0.85281, saving model to ../model/emsemble_final_EfficientNetB3_fold_4.hdf5\n",
      "Epoch 11/150\n",
      "502/502 [==============================] - 233s 464ms/step - loss: 0.8328 - acc: 0.8703 - f1_m: 0.8503 - val_loss: 0.3839 - val_acc: 0.8790 - val_f1_m: 0.8768\n",
      "\n",
      "Epoch 00011: val_f1_m improved from 0.85281 to 0.87681, saving model to ../model/emsemble_final_EfficientNetB3_fold_4.hdf5\n",
      "Epoch 12/150\n",
      "502/502 [==============================] - 232s 462ms/step - loss: 0.7976 - acc: 0.8811 - f1_m: 0.8646 - val_loss: 0.4855 - val_acc: 0.8686 - val_f1_m: 0.8812\n",
      "\n",
      "Epoch 00012: val_f1_m improved from 0.87681 to 0.88122, saving model to ../model/emsemble_final_EfficientNetB3_fold_4.hdf5\n",
      "Epoch 13/150\n",
      "502/502 [==============================] - 233s 464ms/step - loss: 0.7378 - acc: 0.9014 - f1_m: 0.8829 - val_loss: 0.3855 - val_acc: 0.8911 - val_f1_m: 0.8845\n",
      "\n",
      "Epoch 00013: val_f1_m improved from 0.88122 to 0.88447, saving model to ../model/emsemble_final_EfficientNetB3_fold_4.hdf5\n",
      "Epoch 14/150\n",
      "502/502 [==============================] - 233s 464ms/step - loss: 0.7082 - acc: 0.9094 - f1_m: 0.8996 - val_loss: 0.4477 - val_acc: 0.8669 - val_f1_m: 0.8787\n",
      "\n",
      "Epoch 00014: val_f1_m did not improve from 0.88447\n",
      "Epoch 15/150\n",
      "502/502 [==============================] - 233s 464ms/step - loss: 0.6739 - acc: 0.9212 - f1_m: 0.9080 - val_loss: 0.4278 - val_acc: 0.8710 - val_f1_m: 0.8703\n",
      "\n",
      "Epoch 00015: val_f1_m did not improve from 0.88447\n",
      "Epoch 16/150\n",
      "502/502 [==============================] - 233s 464ms/step - loss: 0.6548 - acc: 0.9263 - f1_m: 0.9145 - val_loss: 0.3258 - val_acc: 0.9158 - val_f1_m: 0.9029\n",
      "\n",
      "Epoch 00016: val_f1_m improved from 0.88447 to 0.90288, saving model to ../model/emsemble_final_EfficientNetB3_fold_4.hdf5\n",
      "Epoch 17/150\n",
      "502/502 [==============================] - 233s 464ms/step - loss: 0.6211 - acc: 0.9379 - f1_m: 0.9246 - val_loss: 0.3629 - val_acc: 0.8871 - val_f1_m: 0.8882\n",
      "\n",
      "Epoch 00017: val_f1_m did not improve from 0.90288\n",
      "Epoch 18/150\n",
      "502/502 [==============================] - 233s 464ms/step - loss: 0.6011 - acc: 0.9443 - f1_m: 0.9306 - val_loss: 0.3688 - val_acc: 0.8871 - val_f1_m: 0.8971\n",
      "\n",
      "Epoch 00018: val_f1_m did not improve from 0.90288\n",
      "Epoch 19/150\n",
      "502/502 [==============================] - 233s 464ms/step - loss: 0.5794 - acc: 0.9488 - f1_m: 0.9353 - val_loss: 0.3337 - val_acc: 0.9012 - val_f1_m: 0.9025\n",
      "\n",
      "Epoch 00019: val_f1_m did not improve from 0.90288\n",
      "Epoch 20/150\n",
      "502/502 [==============================] - 232s 461ms/step - loss: 0.5674 - acc: 0.9506 - f1_m: 0.9398 - val_loss: 0.4192 - val_acc: 0.8850 - val_f1_m: 0.8856\n",
      "\n",
      "Epoch 00020: val_f1_m did not improve from 0.90288\n",
      "Epoch 21/150\n",
      "502/502 [==============================] - 232s 462ms/step - loss: 0.5559 - acc: 0.9573 - f1_m: 0.9461 - val_loss: 0.3853 - val_acc: 0.8851 - val_f1_m: 0.8903\n",
      "\n",
      "Epoch 00021: val_f1_m did not improve from 0.90288\n",
      "Epoch 22/150\n",
      "502/502 [==============================] - 233s 464ms/step - loss: 0.5337 - acc: 0.9598 - f1_m: 0.9486 - val_loss: 0.3517 - val_acc: 0.9153 - val_f1_m: 0.9139\n",
      "\n",
      "Epoch 00022: val_f1_m improved from 0.90288 to 0.91390, saving model to ../model/emsemble_final_EfficientNetB3_fold_4.hdf5\n",
      "Epoch 23/150\n",
      "502/502 [==============================] - 233s 463ms/step - loss: 0.5281 - acc: 0.9639 - f1_m: 0.9525 - val_loss: 0.3001 - val_acc: 0.9234 - val_f1_m: 0.9172\n",
      "\n",
      "Epoch 00023: val_f1_m improved from 0.91390 to 0.91722, saving model to ../model/emsemble_final_EfficientNetB3_fold_4.hdf5\n",
      "Epoch 24/150\n",
      "502/502 [==============================] - 233s 463ms/step - loss: 0.5181 - acc: 0.9649 - f1_m: 0.9554 - val_loss: 0.3185 - val_acc: 0.9363 - val_f1_m: 0.9134\n",
      "\n",
      "Epoch 00024: val_f1_m did not improve from 0.91722\n",
      "Epoch 25/150\n",
      "502/502 [==============================] - 233s 464ms/step - loss: 0.5062 - acc: 0.9690 - f1_m: 0.9578 - val_loss: 0.3760 - val_acc: 0.9194 - val_f1_m: 0.9060\n",
      "\n",
      "Epoch 00025: val_f1_m did not improve from 0.91722\n",
      "Epoch 26/150\n",
      "502/502 [==============================] - 233s 464ms/step - loss: 0.5029 - acc: 0.9704 - f1_m: 0.9601 - val_loss: 0.3970 - val_acc: 0.9093 - val_f1_m: 0.9046\n",
      "\n",
      "Epoch 00026: val_f1_m did not improve from 0.91722\n",
      "Epoch 27/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "502/502 [==============================] - 233s 464ms/step - loss: 0.4962 - acc: 0.9690 - f1_m: 0.9620 - val_loss: 0.2975 - val_acc: 0.9254 - val_f1_m: 0.9209\n",
      "\n",
      "Epoch 00027: val_f1_m improved from 0.91722 to 0.92091, saving model to ../model/emsemble_final_EfficientNetB3_fold_4.hdf5\n",
      "Epoch 28/150\n",
      "502/502 [==============================] - 232s 462ms/step - loss: 0.4860 - acc: 0.9709 - f1_m: 0.9639 - val_loss: 0.3537 - val_acc: 0.9261 - val_f1_m: 0.9221\n",
      "\n",
      "Epoch 00028: val_f1_m improved from 0.92091 to 0.92211, saving model to ../model/emsemble_final_EfficientNetB3_fold_4.hdf5\n",
      "Epoch 29/150\n",
      "502/502 [==============================] - 233s 464ms/step - loss: 0.4714 - acc: 0.9781 - f1_m: 0.9680 - val_loss: 0.3570 - val_acc: 0.9093 - val_f1_m: 0.9063\n",
      "\n",
      "Epoch 00029: val_f1_m did not improve from 0.92211\n",
      "Epoch 30/150\n",
      "502/502 [==============================] - 232s 462ms/step - loss: 0.4648 - acc: 0.9772 - f1_m: 0.9695 - val_loss: 0.3506 - val_acc: 0.9153 - val_f1_m: 0.9138\n",
      "\n",
      "Epoch 00030: val_f1_m did not improve from 0.92211\n",
      "Epoch 31/150\n",
      "502/502 [==============================] - 233s 464ms/step - loss: 0.4592 - acc: 0.9783 - f1_m: 0.9707 - val_loss: 0.3220 - val_acc: 0.9274 - val_f1_m: 0.9259\n",
      "\n",
      "Epoch 00031: val_f1_m improved from 0.92211 to 0.92586, saving model to ../model/emsemble_final_EfficientNetB3_fold_4.hdf5\n",
      "Epoch 32/150\n",
      "502/502 [==============================] - 233s 464ms/step - loss: 0.4485 - acc: 0.9824 - f1_m: 0.9741 - val_loss: 0.3646 - val_acc: 0.9097 - val_f1_m: 0.9029\n",
      "\n",
      "Epoch 00032: val_f1_m did not improve from 0.92586\n",
      "Epoch 33/150\n",
      "502/502 [==============================] - 233s 464ms/step - loss: 0.4615 - acc: 0.9796 - f1_m: 0.9721 - val_loss: 0.4048 - val_acc: 0.9012 - val_f1_m: 0.8936\n",
      "\n",
      "Epoch 00033: val_f1_m did not improve from 0.92586\n",
      "Epoch 34/150\n",
      "502/502 [==============================] - 232s 462ms/step - loss: 0.4453 - acc: 0.9801 - f1_m: 0.9744 - val_loss: 0.2959 - val_acc: 0.9415 - val_f1_m: 0.9233\n",
      "\n",
      "Epoch 00034: val_f1_m did not improve from 0.92586\n",
      "Epoch 35/150\n",
      "502/502 [==============================] - 233s 463ms/step - loss: 0.4492 - acc: 0.9807 - f1_m: 0.9736 - val_loss: 0.2941 - val_acc: 0.9335 - val_f1_m: 0.9269\n",
      "\n",
      "Epoch 00035: val_f1_m improved from 0.92586 to 0.92686, saving model to ../model/emsemble_final_EfficientNetB3_fold_4.hdf5\n",
      "Epoch 36/150\n",
      "502/502 [==============================] - 232s 461ms/step - loss: 0.4387 - acc: 0.9822 - f1_m: 0.9754 - val_loss: 0.3839 - val_acc: 0.9199 - val_f1_m: 0.9185\n",
      "\n",
      "Epoch 00036: val_f1_m did not improve from 0.92686\n",
      "Epoch 37/150\n",
      "502/502 [==============================] - 233s 464ms/step - loss: 0.4305 - acc: 0.9837 - f1_m: 0.9790 - val_loss: 0.3663 - val_acc: 0.9234 - val_f1_m: 0.9199\n",
      "\n",
      "Epoch 00037: val_f1_m did not improve from 0.92686\n",
      "Epoch 38/150\n",
      "502/502 [==============================] - 233s 464ms/step - loss: 0.4296 - acc: 0.9841 - f1_m: 0.9767 - val_loss: 0.2754 - val_acc: 0.9335 - val_f1_m: 0.9278\n",
      "\n",
      "Epoch 00038: val_f1_m improved from 0.92686 to 0.92781, saving model to ../model/emsemble_final_EfficientNetB3_fold_4.hdf5\n",
      "Epoch 39/150\n",
      "502/502 [==============================] - 233s 463ms/step - loss: 0.4288 - acc: 0.9864 - f1_m: 0.9795 - val_loss: 0.2862 - val_acc: 0.9395 - val_f1_m: 0.9300\n",
      "\n",
      "Epoch 00039: val_f1_m improved from 0.92781 to 0.92995, saving model to ../model/emsemble_final_EfficientNetB3_fold_4.hdf5\n",
      "Epoch 40/150\n",
      "502/502 [==============================] - 233s 464ms/step - loss: 0.4231 - acc: 0.9880 - f1_m: 0.9811 - val_loss: 0.3972 - val_acc: 0.9117 - val_f1_m: 0.8978\n",
      "\n",
      "Epoch 00040: val_f1_m did not improve from 0.92995\n",
      "Epoch 41/150\n",
      "502/502 [==============================] - 233s 464ms/step - loss: 0.4190 - acc: 0.9872 - f1_m: 0.9824 - val_loss: 0.3082 - val_acc: 0.9274 - val_f1_m: 0.9240\n",
      "\n",
      "Epoch 00041: val_f1_m did not improve from 0.92995\n",
      "Epoch 42/150\n",
      "502/502 [==============================] - 232s 462ms/step - loss: 0.4220 - acc: 0.9867 - f1_m: 0.9810 - val_loss: 0.3437 - val_acc: 0.9254 - val_f1_m: 0.9146\n",
      "\n",
      "Epoch 00042: val_f1_m did not improve from 0.92995\n",
      "Epoch 43/150\n",
      "502/502 [==============================] - 232s 462ms/step - loss: 0.4105 - acc: 0.9874 - f1_m: 0.9821 - val_loss: 0.3488 - val_acc: 0.9194 - val_f1_m: 0.9189\n",
      "\n",
      "Epoch 00043: val_f1_m did not improve from 0.92995\n",
      "Epoch 44/150\n",
      "502/502 [==============================] - 232s 462ms/step - loss: 0.4070 - acc: 0.9877 - f1_m: 0.9835 - val_loss: 0.2763 - val_acc: 0.9343 - val_f1_m: 0.9235\n",
      "\n",
      "Epoch 00044: val_f1_m did not improve from 0.92995\n",
      "Epoch 45/150\n",
      "502/502 [==============================] - 232s 462ms/step - loss: 0.4099 - acc: 0.9884 - f1_m: 0.9834 - val_loss: 0.3555 - val_acc: 0.9173 - val_f1_m: 0.9130\n",
      "\n",
      "Epoch 00045: val_f1_m did not improve from 0.92995\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Epoch 46/150\n",
      "502/502 [==============================] - 232s 462ms/step - loss: 0.3974 - acc: 0.9912 - f1_m: 0.9868 - val_loss: 0.2817 - val_acc: 0.9315 - val_f1_m: 0.9254\n",
      "\n",
      "Epoch 00046: val_f1_m did not improve from 0.92995\n",
      "Epoch 47/150\n",
      "502/502 [==============================] - 233s 464ms/step - loss: 0.3910 - acc: 0.9924 - f1_m: 0.9876 - val_loss: 0.2644 - val_acc: 0.9375 - val_f1_m: 0.9366\n",
      "\n",
      "Epoch 00047: val_f1_m improved from 0.92995 to 0.93662, saving model to ../model/emsemble_final_EfficientNetB3_fold_4.hdf5\n",
      "Epoch 48/150\n",
      "502/502 [==============================] - 233s 463ms/step - loss: 0.3872 - acc: 0.9935 - f1_m: 0.9884 - val_loss: 0.3617 - val_acc: 0.9199 - val_f1_m: 0.9140\n",
      "\n",
      "Epoch 00048: val_f1_m did not improve from 0.93662\n",
      "Epoch 49/150\n",
      "502/502 [==============================] - 233s 464ms/step - loss: 0.3880 - acc: 0.9928 - f1_m: 0.9878 - val_loss: 0.3145 - val_acc: 0.9214 - val_f1_m: 0.9234\n",
      "\n",
      "Epoch 00049: val_f1_m did not improve from 0.93662\n",
      "Epoch 50/150\n",
      "502/502 [==============================] - 233s 464ms/step - loss: 0.3830 - acc: 0.9941 - f1_m: 0.9901 - val_loss: 0.2913 - val_acc: 0.9435 - val_f1_m: 0.9334\n",
      "\n",
      "Epoch 00050: val_f1_m did not improve from 0.93662\n",
      "Epoch 51/150\n",
      "502/502 [==============================] - 232s 462ms/step - loss: 0.3789 - acc: 0.9939 - f1_m: 0.9902 - val_loss: 0.3088 - val_acc: 0.9335 - val_f1_m: 0.9215\n",
      "\n",
      "Epoch 00051: val_f1_m did not improve from 0.93662\n",
      "Epoch 52/150\n",
      "502/502 [==============================] - 233s 464ms/step - loss: 0.3753 - acc: 0.9943 - f1_m: 0.9912 - val_loss: 0.3541 - val_acc: 0.9199 - val_f1_m: 0.9055\n",
      "\n",
      "Epoch 00052: val_f1_m did not improve from 0.93662\n",
      "Epoch 53/150\n",
      "502/502 [==============================] - 233s 464ms/step - loss: 0.3761 - acc: 0.9929 - f1_m: 0.9893 - val_loss: 0.3190 - val_acc: 0.9234 - val_f1_m: 0.9230\n",
      "\n",
      "Epoch 00053: val_f1_m did not improve from 0.93662\n",
      "Epoch 54/150\n",
      "502/502 [==============================] - 233s 463ms/step - loss: 0.3754 - acc: 0.9948 - f1_m: 0.9904 - val_loss: 0.3467 - val_acc: 0.9274 - val_f1_m: 0.9160\n",
      "\n",
      "Epoch 00054: val_f1_m did not improve from 0.93662\n",
      "\n",
      "Epoch 00054: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 55/150\n",
      "502/502 [==============================] - 233s 463ms/step - loss: 0.3745 - acc: 0.9940 - f1_m: 0.9914 - val_loss: 0.2764 - val_acc: 0.9476 - val_f1_m: 0.9381\n",
      "\n",
      "Epoch 00055: val_f1_m improved from 0.93662 to 0.93808, saving model to ../model/emsemble_final_EfficientNetB3_fold_4.hdf5\n",
      "Epoch 56/150\n",
      "502/502 [==============================] - 233s 464ms/step - loss: 0.3739 - acc: 0.9930 - f1_m: 0.9906 - val_loss: 0.3085 - val_acc: 0.9322 - val_f1_m: 0.9308\n",
      "\n",
      "Epoch 00056: val_f1_m did not improve from 0.93808\n",
      "Epoch 57/150\n",
      "502/502 [==============================] - 233s 464ms/step - loss: 0.3693 - acc: 0.9950 - f1_m: 0.9916 - val_loss: 0.2222 - val_acc: 0.9556 - val_f1_m: 0.9459\n",
      "\n",
      "Epoch 00057: val_f1_m improved from 0.93808 to 0.94593, saving model to ../model/emsemble_final_EfficientNetB3_fold_4.hdf5\n",
      "Epoch 58/150\n",
      "502/502 [==============================] - 232s 461ms/step - loss: 0.3704 - acc: 0.9949 - f1_m: 0.9914 - val_loss: 0.3228 - val_acc: 0.9375 - val_f1_m: 0.9339\n",
      "\n",
      "Epoch 00058: val_f1_m did not improve from 0.94593\n",
      "Epoch 59/150\n",
      "502/502 [==============================] - 232s 462ms/step - loss: 0.3684 - acc: 0.9940 - f1_m: 0.9914 - val_loss: 0.3272 - val_acc: 0.9294 - val_f1_m: 0.9237\n",
      "\n",
      "Epoch 00059: val_f1_m did not improve from 0.94593\n",
      "Epoch 60/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "502/502 [==============================] - 232s 462ms/step - loss: 0.3681 - acc: 0.9955 - f1_m: 0.9930 - val_loss: 0.3322 - val_acc: 0.9199 - val_f1_m: 0.9128\n",
      "\n",
      "Epoch 00060: val_f1_m did not improve from 0.94593\n",
      "Epoch 61/150\n",
      "502/502 [==============================] - 233s 464ms/step - loss: 0.3677 - acc: 0.9954 - f1_m: 0.9927 - val_loss: 0.3173 - val_acc: 0.9294 - val_f1_m: 0.9239\n",
      "\n",
      "Epoch 00061: val_f1_m did not improve from 0.94593\n",
      "Epoch 62/150\n",
      "502/502 [==============================] - 233s 464ms/step - loss: 0.3646 - acc: 0.9966 - f1_m: 0.9941 - val_loss: 0.3514 - val_acc: 0.9395 - val_f1_m: 0.9307\n",
      "\n",
      "Epoch 00062: val_f1_m did not improve from 0.94593\n",
      "Epoch 63/150\n",
      "502/502 [==============================] - 233s 464ms/step - loss: 0.3582 - acc: 0.9960 - f1_m: 0.9935 - val_loss: 0.2742 - val_acc: 0.9415 - val_f1_m: 0.9371\n",
      "\n",
      "Epoch 00063: val_f1_m did not improve from 0.94593\n",
      "Epoch 64/150\n",
      "502/502 [==============================] - 233s 464ms/step - loss: 0.3640 - acc: 0.9948 - f1_m: 0.9918 - val_loss: 0.2489 - val_acc: 0.9425 - val_f1_m: 0.9381\n",
      "\n",
      "Epoch 00064: val_f1_m did not improve from 0.94593\n",
      "\n",
      "Epoch 00064: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Epoch 65/150\n",
      "502/502 [==============================] - 232s 462ms/step - loss: 0.3642 - acc: 0.9961 - f1_m: 0.9939 - val_loss: 0.3073 - val_acc: 0.9335 - val_f1_m: 0.9289\n",
      "\n",
      "Epoch 00065: val_f1_m did not improve from 0.94593\n",
      "Epoch 66/150\n",
      "502/502 [==============================] - 232s 461ms/step - loss: 0.3622 - acc: 0.9973 - f1_m: 0.9948 - val_loss: 0.3205 - val_acc: 0.9315 - val_f1_m: 0.9343\n",
      "\n",
      "Epoch 00066: val_f1_m did not improve from 0.94593\n",
      "Epoch 67/150\n",
      "502/502 [==============================] - 232s 462ms/step - loss: 0.3612 - acc: 0.9946 - f1_m: 0.9927 - val_loss: 0.3209 - val_acc: 0.9294 - val_f1_m: 0.9160\n",
      "\n",
      "Epoch 00067: val_f1_m did not improve from 0.94593\n",
      "Epoch 68/150\n",
      "502/502 [==============================] - 232s 461ms/step - loss: 0.3647 - acc: 0.9960 - f1_m: 0.9934 - val_loss: 0.3104 - val_acc: 0.9302 - val_f1_m: 0.9315\n",
      "\n",
      "Epoch 00068: val_f1_m did not improve from 0.94593\n",
      "Epoch 69/150\n",
      "502/502 [==============================] - 233s 464ms/step - loss: 0.3639 - acc: 0.9953 - f1_m: 0.9914 - val_loss: 0.3059 - val_acc: 0.9355 - val_f1_m: 0.9309\n",
      "\n",
      "Epoch 00069: val_f1_m did not improve from 0.94593\n",
      "Epoch 70/150\n",
      "502/502 [==============================] - 232s 462ms/step - loss: 0.3599 - acc: 0.9961 - f1_m: 0.9933 - val_loss: 0.3305 - val_acc: 0.9335 - val_f1_m: 0.9210\n",
      "\n",
      "Epoch 00070: val_f1_m did not improve from 0.94593\n",
      "Epoch 71/150\n",
      "502/502 [==============================] - 232s 462ms/step - loss: 0.3582 - acc: 0.9968 - f1_m: 0.9956 - val_loss: 0.2750 - val_acc: 0.9395 - val_f1_m: 0.9406\n",
      "\n",
      "Epoch 00071: val_f1_m did not improve from 0.94593\n",
      "\n",
      "Epoch 00071: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "Epoch 72/150\n",
      "502/502 [==============================] - 232s 462ms/step - loss: 0.3613 - acc: 0.9966 - f1_m: 0.9941 - val_loss: 0.2989 - val_acc: 0.9405 - val_f1_m: 0.9300\n",
      "\n",
      "Epoch 00072: val_f1_m did not improve from 0.94593\n",
      "Epoch 73/150\n",
      "502/502 [==============================] - 232s 462ms/step - loss: 0.3589 - acc: 0.9959 - f1_m: 0.9935 - val_loss: 0.3329 - val_acc: 0.9254 - val_f1_m: 0.9230\n",
      "\n",
      "Epoch 00073: val_f1_m did not improve from 0.94593\n",
      "Epoch 74/150\n",
      "502/502 [==============================] - 232s 461ms/step - loss: 0.3587 - acc: 0.9956 - f1_m: 0.9933 - val_loss: 0.2549 - val_acc: 0.9415 - val_f1_m: 0.9389\n",
      "\n",
      "Epoch 00074: val_f1_m did not improve from 0.94593\n",
      "Epoch 75/150\n",
      "502/502 [==============================] - 232s 462ms/step - loss: 0.3601 - acc: 0.9963 - f1_m: 0.9943 - val_loss: 0.3374 - val_acc: 0.9355 - val_f1_m: 0.9284\n",
      "\n",
      "Epoch 00075: val_f1_m did not improve from 0.94593\n",
      "Epoch 76/150\n",
      "502/502 [==============================] - 232s 461ms/step - loss: 0.3611 - acc: 0.9961 - f1_m: 0.9935 - val_loss: 0.2981 - val_acc: 0.9343 - val_f1_m: 0.9258\n",
      "\n",
      "Epoch 00076: val_f1_m did not improve from 0.94593\n",
      "Epoch 77/150\n",
      "502/502 [==============================] - 233s 464ms/step - loss: 0.3531 - acc: 0.9974 - f1_m: 0.9946 - val_loss: 0.2818 - val_acc: 0.9355 - val_f1_m: 0.9290\n",
      "\n",
      "Epoch 00077: val_f1_m did not improve from 0.94593\n",
      "Epoch 78/150\n",
      "502/502 [==============================] - 232s 462ms/step - loss: 0.3584 - acc: 0.9958 - f1_m: 0.9929 - val_loss: 0.2715 - val_acc: 0.9516 - val_f1_m: 0.9418\n",
      "\n",
      "Epoch 00078: val_f1_m did not improve from 0.94593\n",
      "Epoch 79/150\n",
      "502/502 [==============================] - 232s 462ms/step - loss: 0.3565 - acc: 0.9966 - f1_m: 0.9947 - val_loss: 0.3707 - val_acc: 0.9335 - val_f1_m: 0.9265\n",
      "\n",
      "Epoch 00079: val_f1_m did not improve from 0.94593\n",
      "Epoch 80/150\n",
      "502/502 [==============================] - 232s 462ms/step - loss: 0.3562 - acc: 0.9966 - f1_m: 0.9936 - val_loss: 0.2979 - val_acc: 0.9425 - val_f1_m: 0.9426\n",
      "\n",
      "Epoch 00080: val_f1_m did not improve from 0.94593\n",
      "Epoch 81/150\n",
      "502/502 [==============================] - 232s 462ms/step - loss: 0.3561 - acc: 0.9965 - f1_m: 0.9938 - val_loss: 0.3079 - val_acc: 0.9375 - val_f1_m: 0.9291\n",
      "\n",
      "Epoch 00081: val_f1_m did not improve from 0.94593\n",
      "Epoch 82/150\n",
      "502/502 [==============================] - 232s 463ms/step - loss: 0.3589 - acc: 0.9956 - f1_m: 0.9942 - val_loss: 0.3094 - val_acc: 0.9375 - val_f1_m: 0.9286\n",
      "\n",
      "Epoch 00082: val_f1_m did not improve from 0.94593\n",
      "Epoch 00082: early stopping\n",
      "[*]-------------------EfficientNetB3 model_foldstep_4-------------------\n",
      ">>image_size: 299\n",
      ">>train_path: ../input/train_crop_299\n",
      ">>test_path: ../input/test_crop_299\n",
      "Found 8078 validated image filenames belonging to 196 classes.\n",
      "Found 8078 validated image filenames belonging to 196 classes.\n",
      "Found 1912 validated image filenames belonging to 196 classes.\n",
      "Found 6150 validated image filenames.\n",
      ">>model path to save: ../model/emsemble_final_EfficientNetB3_fold_5.hdf5\n",
      "Epoch 1/150\n",
      "505/505 [==============================] - 257s 509ms/step - loss: 5.1730 - acc: 0.0208 - f1_m: 0.0000e+00 - val_loss: 4.4788 - val_acc: 0.1208 - val_f1_m: 0.0039\n",
      "\n",
      "Epoch 00001: val_f1_m improved from -inf to 0.00392, saving model to ../model/emsemble_final_EfficientNetB3_fold_5.hdf5\n",
      "Epoch 2/150\n",
      "505/505 [==============================] - 232s 459ms/step - loss: 4.1137 - acc: 0.1408 - f1_m: 0.0178 - val_loss: 2.8309 - val_acc: 0.3396 - val_f1_m: 0.1092\n",
      "\n",
      "Epoch 00002: val_f1_m improved from 0.00392 to 0.10919, saving model to ../model/emsemble_final_EfficientNetB3_fold_5.hdf5\n",
      "Epoch 3/150\n",
      "505/505 [==============================] - 232s 460ms/step - loss: 2.9638 - acc: 0.3363 - f1_m: 0.1395 - val_loss: 1.6143 - val_acc: 0.6104 - val_f1_m: 0.4588\n",
      "\n",
      "Epoch 00003: val_f1_m improved from 0.10919 to 0.45876, saving model to ../model/emsemble_final_EfficientNetB3_fold_5.hdf5\n",
      "Epoch 4/150\n",
      "505/505 [==============================] - 232s 459ms/step - loss: 2.2166 - acc: 0.5006 - f1_m: 0.3651 - val_loss: 1.1608 - val_acc: 0.6843 - val_f1_m: 0.6672\n",
      "\n",
      "Epoch 00004: val_f1_m improved from 0.45876 to 0.66716, saving model to ../model/emsemble_final_EfficientNetB3_fold_5.hdf5\n",
      "Epoch 5/150\n",
      "505/505 [==============================] - 231s 457ms/step - loss: 1.7655 - acc: 0.6018 - f1_m: 0.5272 - val_loss: 0.9664 - val_acc: 0.7229 - val_f1_m: 0.7266\n",
      "\n",
      "Epoch 00005: val_f1_m improved from 0.66716 to 0.72663, saving model to ../model/emsemble_final_EfficientNetB3_fold_5.hdf5\n",
      "Epoch 6/150\n",
      "505/505 [==============================] - 232s 459ms/step - loss: 1.4577 - acc: 0.6899 - f1_m: 0.6398 - val_loss: 0.7401 - val_acc: 0.7750 - val_f1_m: 0.7645\n",
      "\n",
      "Epoch 00006: val_f1_m improved from 0.72663 to 0.76449, saving model to ../model/emsemble_final_EfficientNetB3_fold_5.hdf5\n",
      "Epoch 7/150\n",
      "505/505 [==============================] - 232s 460ms/step - loss: 1.2422 - acc: 0.7530 - f1_m: 0.7113 - val_loss: 0.6740 - val_acc: 0.8104 - val_f1_m: 0.8165\n",
      "\n",
      "Epoch 00007: val_f1_m improved from 0.76449 to 0.81652, saving model to ../model/emsemble_final_EfficientNetB3_fold_5.hdf5\n",
      "Epoch 8/150\n",
      "505/505 [==============================] - 231s 457ms/step - loss: 1.1211 - acc: 0.7937 - f1_m: 0.7597 - val_loss: 0.4995 - val_acc: 0.8411 - val_f1_m: 0.8431\n",
      "\n",
      "Epoch 00008: val_f1_m improved from 0.81652 to 0.84308, saving model to ../model/emsemble_final_EfficientNetB3_fold_5.hdf5\n",
      "Epoch 9/150\n",
      "505/505 [==============================] - 232s 460ms/step - loss: 1.0123 - acc: 0.8134 - f1_m: 0.7887 - val_loss: 0.5317 - val_acc: 0.8396 - val_f1_m: 0.8320\n",
      "\n",
      "Epoch 00009: val_f1_m did not improve from 0.84308\n",
      "Epoch 10/150\n",
      "505/505 [==============================] - 232s 459ms/step - loss: 0.9022 - acc: 0.8482 - f1_m: 0.8271 - val_loss: 0.4488 - val_acc: 0.8646 - val_f1_m: 0.8639\n",
      "\n",
      "Epoch 00010: val_f1_m improved from 0.84308 to 0.86388, saving model to ../model/emsemble_final_EfficientNetB3_fold_5.hdf5\n",
      "Epoch 11/150\n",
      "505/505 [==============================] - 231s 457ms/step - loss: 0.8578 - acc: 0.8611 - f1_m: 0.8412 - val_loss: 0.4618 - val_acc: 0.8854 - val_f1_m: 0.8754\n",
      "\n",
      "Epoch 00011: val_f1_m improved from 0.86388 to 0.87538, saving model to ../model/emsemble_final_EfficientNetB3_fold_5.hdf5\n",
      "Epoch 12/150\n",
      "505/505 [==============================] - 232s 459ms/step - loss: 0.7863 - acc: 0.8864 - f1_m: 0.8669 - val_loss: 0.4527 - val_acc: 0.8644 - val_f1_m: 0.8774\n",
      "\n",
      "Epoch 00012: val_f1_m improved from 0.87538 to 0.87738, saving model to ../model/emsemble_final_EfficientNetB3_fold_5.hdf5\n",
      "Epoch 13/150\n",
      "505/505 [==============================] - 232s 459ms/step - loss: 0.7486 - acc: 0.8951 - f1_m: 0.8834 - val_loss: 0.4314 - val_acc: 0.8604 - val_f1_m: 0.8782\n",
      "\n",
      "Epoch 00013: val_f1_m improved from 0.87738 to 0.87819, saving model to ../model/emsemble_final_EfficientNetB3_fold_5.hdf5\n",
      "Epoch 14/150\n",
      "505/505 [==============================] - 232s 459ms/step - loss: 0.7083 - acc: 0.9084 - f1_m: 0.8940 - val_loss: 0.3860 - val_acc: 0.8896 - val_f1_m: 0.8998\n",
      "\n",
      "Epoch 00014: val_f1_m improved from 0.87819 to 0.89976, saving model to ../model/emsemble_final_EfficientNetB3_fold_5.hdf5\n",
      "Epoch 15/150\n",
      "505/505 [==============================] - 232s 460ms/step - loss: 0.6853 - acc: 0.9128 - f1_m: 0.9009 - val_loss: 0.3888 - val_acc: 0.8958 - val_f1_m: 0.8902\n",
      "\n",
      "Epoch 00015: val_f1_m did not improve from 0.89976\n",
      "Epoch 16/150\n",
      "505/505 [==============================] - 232s 459ms/step - loss: 0.6418 - acc: 0.9306 - f1_m: 0.9153 - val_loss: 0.3768 - val_acc: 0.8919 - val_f1_m: 0.8943\n",
      "\n",
      "Epoch 00016: val_f1_m did not improve from 0.89976\n",
      "Epoch 17/150\n",
      "505/505 [==============================] - 232s 459ms/step - loss: 0.6255 - acc: 0.9348 - f1_m: 0.9221 - val_loss: 0.3867 - val_acc: 0.8917 - val_f1_m: 0.8951\n",
      "\n",
      "Epoch 00017: val_f1_m did not improve from 0.89976\n",
      "Epoch 18/150\n",
      "505/505 [==============================] - 232s 459ms/step - loss: 0.6081 - acc: 0.9414 - f1_m: 0.9283 - val_loss: 0.4547 - val_acc: 0.8729 - val_f1_m: 0.8754\n",
      "\n",
      "Epoch 00018: val_f1_m did not improve from 0.89976\n",
      "Epoch 19/150\n",
      "505/505 [==============================] - 231s 457ms/step - loss: 0.5883 - acc: 0.9463 - f1_m: 0.9357 - val_loss: 0.3377 - val_acc: 0.8979 - val_f1_m: 0.9026\n",
      "\n",
      "Epoch 00019: val_f1_m improved from 0.89976 to 0.90258, saving model to ../model/emsemble_final_EfficientNetB3_fold_5.hdf5\n",
      "Epoch 20/150\n",
      "505/505 [==============================] - 231s 457ms/step - loss: 0.5605 - acc: 0.9558 - f1_m: 0.9453 - val_loss: 0.4417 - val_acc: 0.8941 - val_f1_m: 0.8965\n",
      "\n",
      "Epoch 00020: val_f1_m did not improve from 0.90258\n",
      "Epoch 21/150\n",
      "505/505 [==============================] - 232s 460ms/step - loss: 0.5559 - acc: 0.9548 - f1_m: 0.9448 - val_loss: 0.3576 - val_acc: 0.9021 - val_f1_m: 0.9047\n",
      "\n",
      "Epoch 00021: val_f1_m improved from 0.90258 to 0.90471, saving model to ../model/emsemble_final_EfficientNetB3_fold_5.hdf5\n",
      "Epoch 22/150\n",
      "505/505 [==============================] - 232s 459ms/step - loss: 0.5413 - acc: 0.9588 - f1_m: 0.9481 - val_loss: 0.3142 - val_acc: 0.9062 - val_f1_m: 0.9111\n",
      "\n",
      "Epoch 00022: val_f1_m improved from 0.90471 to 0.91111, saving model to ../model/emsemble_final_EfficientNetB3_fold_5.hdf5\n",
      "Epoch 23/150\n",
      "505/505 [==============================] - 232s 460ms/step - loss: 0.5320 - acc: 0.9645 - f1_m: 0.9514 - val_loss: 0.4487 - val_acc: 0.8812 - val_f1_m: 0.8748\n",
      "\n",
      "Epoch 00023: val_f1_m did not improve from 0.91111\n",
      "Epoch 24/150\n",
      "505/505 [==============================] - 232s 459ms/step - loss: 0.5237 - acc: 0.9646 - f1_m: 0.9548 - val_loss: 0.4241 - val_acc: 0.8814 - val_f1_m: 0.8874\n",
      "\n",
      "Epoch 00024: val_f1_m did not improve from 0.91111\n",
      "Epoch 25/150\n",
      "505/505 [==============================] - 232s 460ms/step - loss: 0.5065 - acc: 0.9693 - f1_m: 0.9588 - val_loss: 0.4709 - val_acc: 0.8917 - val_f1_m: 0.8888\n",
      "\n",
      "Epoch 00025: val_f1_m did not improve from 0.91111\n",
      "Epoch 26/150\n",
      "505/505 [==============================] - 232s 459ms/step - loss: 0.4991 - acc: 0.9705 - f1_m: 0.9601 - val_loss: 0.3566 - val_acc: 0.8854 - val_f1_m: 0.8872\n",
      "\n",
      "Epoch 00026: val_f1_m did not improve from 0.91111\n",
      "Epoch 27/150\n",
      "505/505 [==============================] - 232s 459ms/step - loss: 0.5007 - acc: 0.9699 - f1_m: 0.9602 - val_loss: 0.3714 - val_acc: 0.9125 - val_f1_m: 0.9130\n",
      "\n",
      "Epoch 00027: val_f1_m improved from 0.91111 to 0.91303, saving model to ../model/emsemble_final_EfficientNetB3_fold_5.hdf5\n",
      "Epoch 28/150\n",
      "505/505 [==============================] - 232s 459ms/step - loss: 0.4870 - acc: 0.9730 - f1_m: 0.9651 - val_loss: 0.3222 - val_acc: 0.9174 - val_f1_m: 0.9191\n",
      "\n",
      "Epoch 00028: val_f1_m improved from 0.91303 to 0.91905, saving model to ../model/emsemble_final_EfficientNetB3_fold_5.hdf5\n",
      "Epoch 29/150\n",
      "505/505 [==============================] - 232s 460ms/step - loss: 0.4764 - acc: 0.9746 - f1_m: 0.9661 - val_loss: 0.3294 - val_acc: 0.9208 - val_f1_m: 0.9141\n",
      "\n",
      "Epoch 00029: val_f1_m did not improve from 0.91905\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Epoch 30/150\n",
      "505/505 [==============================] - 232s 459ms/step - loss: 0.4632 - acc: 0.9782 - f1_m: 0.9721 - val_loss: 0.3301 - val_acc: 0.9229 - val_f1_m: 0.9195\n",
      "\n",
      "Epoch 00030: val_f1_m improved from 0.91905 to 0.91950, saving model to ../model/emsemble_final_EfficientNetB3_fold_5.hdf5\n",
      "Epoch 31/150\n",
      "505/505 [==============================] - 232s 459ms/step - loss: 0.4456 - acc: 0.9811 - f1_m: 0.9743 - val_loss: 0.3237 - val_acc: 0.9229 - val_f1_m: 0.9266\n",
      "\n",
      "Epoch 00031: val_f1_m improved from 0.91950 to 0.92663, saving model to ../model/emsemble_final_EfficientNetB3_fold_5.hdf5\n",
      "Epoch 32/150\n",
      "505/505 [==============================] - 232s 459ms/step - loss: 0.4405 - acc: 0.9828 - f1_m: 0.9759 - val_loss: 0.3659 - val_acc: 0.9216 - val_f1_m: 0.9102\n",
      "\n",
      "Epoch 00032: val_f1_m did not improve from 0.92663\n",
      "Epoch 33/150\n",
      "505/505 [==============================] - 232s 459ms/step - loss: 0.4398 - acc: 0.9854 - f1_m: 0.9796 - val_loss: 0.3639 - val_acc: 0.9146 - val_f1_m: 0.9081\n",
      "\n",
      "Epoch 00033: val_f1_m did not improve from 0.92663\n",
      "Epoch 34/150\n",
      "505/505 [==============================] - 232s 459ms/step - loss: 0.4359 - acc: 0.9834 - f1_m: 0.9765 - val_loss: 0.2947 - val_acc: 0.9333 - val_f1_m: 0.9308\n",
      "\n",
      "Epoch 00034: val_f1_m improved from 0.92663 to 0.93081, saving model to ../model/emsemble_final_EfficientNetB3_fold_5.hdf5\n",
      "Epoch 35/150\n",
      "505/505 [==============================] - 232s 459ms/step - loss: 0.4282 - acc: 0.9874 - f1_m: 0.9789 - val_loss: 0.3523 - val_acc: 0.9062 - val_f1_m: 0.9062\n",
      "\n",
      "Epoch 00035: val_f1_m did not improve from 0.93081\n",
      "Epoch 36/150\n",
      "505/505 [==============================] - 232s 459ms/step - loss: 0.4226 - acc: 0.9875 - f1_m: 0.9814 - val_loss: 0.2838 - val_acc: 0.9301 - val_f1_m: 0.9303\n",
      "\n",
      "Epoch 00036: val_f1_m did not improve from 0.93081\n",
      "Epoch 37/150\n",
      "505/505 [==============================] - 232s 459ms/step - loss: 0.4268 - acc: 0.9849 - f1_m: 0.9792 - val_loss: 0.3747 - val_acc: 0.9229 - val_f1_m: 0.9261\n",
      "\n",
      "Epoch 00037: val_f1_m did not improve from 0.93081\n",
      "Epoch 38/150\n",
      "505/505 [==============================] - 232s 459ms/step - loss: 0.4260 - acc: 0.9859 - f1_m: 0.9807 - val_loss: 0.2939 - val_acc: 0.9271 - val_f1_m: 0.9265\n",
      "\n",
      "Epoch 00038: val_f1_m did not improve from 0.93081\n",
      "Epoch 39/150\n",
      "505/505 [==============================] - 232s 459ms/step - loss: 0.4164 - acc: 0.9885 - f1_m: 0.9826 - val_loss: 0.2905 - val_acc: 0.9375 - val_f1_m: 0.9266\n",
      "\n",
      "Epoch 00039: val_f1_m did not improve from 0.93081\n",
      "Epoch 40/150\n",
      "505/505 [==============================] - 232s 459ms/step - loss: 0.4135 - acc: 0.9899 - f1_m: 0.9839 - val_loss: 0.3300 - val_acc: 0.9258 - val_f1_m: 0.9148\n",
      "\n",
      "Epoch 00040: val_f1_m did not improve from 0.93081\n",
      "Epoch 41/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "505/505 [==============================] - 232s 459ms/step - loss: 0.4136 - acc: 0.9877 - f1_m: 0.9832 - val_loss: 0.3377 - val_acc: 0.9167 - val_f1_m: 0.9168\n",
      "\n",
      "Epoch 00041: val_f1_m did not improve from 0.93081\n",
      "Epoch 42/150\n",
      "505/505 [==============================] - 232s 459ms/step - loss: 0.4140 - acc: 0.9884 - f1_m: 0.9819 - val_loss: 0.2628 - val_acc: 0.9417 - val_f1_m: 0.9376\n",
      "\n",
      "Epoch 00042: val_f1_m improved from 0.93081 to 0.93760, saving model to ../model/emsemble_final_EfficientNetB3_fold_5.hdf5\n",
      "Epoch 43/150\n",
      "505/505 [==============================] - 232s 460ms/step - loss: 0.4121 - acc: 0.9866 - f1_m: 0.9825 - val_loss: 0.3965 - val_acc: 0.8938 - val_f1_m: 0.8984\n",
      "\n",
      "Epoch 00043: val_f1_m did not improve from 0.93760\n",
      "Epoch 44/150\n",
      "505/505 [==============================] - 232s 460ms/step - loss: 0.4006 - acc: 0.9892 - f1_m: 0.9834 - val_loss: 0.3231 - val_acc: 0.9174 - val_f1_m: 0.9162\n",
      "\n",
      "Epoch 00044: val_f1_m did not improve from 0.93760\n",
      "Epoch 45/150\n",
      "505/505 [==============================] - 232s 459ms/step - loss: 0.4031 - acc: 0.9918 - f1_m: 0.9857 - val_loss: 0.2697 - val_acc: 0.9396 - val_f1_m: 0.9322\n",
      "\n",
      "Epoch 00045: val_f1_m did not improve from 0.93760\n",
      "Epoch 46/150\n",
      "505/505 [==============================] - 232s 460ms/step - loss: 0.4059 - acc: 0.9888 - f1_m: 0.9842 - val_loss: 0.3107 - val_acc: 0.9292 - val_f1_m: 0.9318\n",
      "\n",
      "Epoch 00046: val_f1_m did not improve from 0.93760\n",
      "Epoch 47/150\n",
      "505/505 [==============================] - 232s 459ms/step - loss: 0.3944 - acc: 0.9920 - f1_m: 0.9878 - val_loss: 0.3821 - val_acc: 0.9146 - val_f1_m: 0.9136\n",
      "\n",
      "Epoch 00047: val_f1_m did not improve from 0.93760\n",
      "Epoch 48/150\n",
      "505/505 [==============================] - 232s 459ms/step - loss: 0.3973 - acc: 0.9898 - f1_m: 0.9849 - val_loss: 0.3223 - val_acc: 0.9237 - val_f1_m: 0.9251\n",
      "\n",
      "Epoch 00048: val_f1_m did not improve from 0.93760\n",
      "Epoch 49/150\n",
      "505/505 [==============================] - 232s 459ms/step - loss: 0.3953 - acc: 0.9911 - f1_m: 0.9866 - val_loss: 0.3138 - val_acc: 0.9500 - val_f1_m: 0.9355\n",
      "\n",
      "Epoch 00049: val_f1_m did not improve from 0.93760\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "Epoch 50/150\n",
      "505/505 [==============================] - 232s 460ms/step - loss: 0.3928 - acc: 0.9918 - f1_m: 0.9873 - val_loss: 0.3684 - val_acc: 0.9104 - val_f1_m: 0.9130\n",
      "\n",
      "Epoch 00050: val_f1_m did not improve from 0.93760\n",
      "Epoch 51/150\n",
      "505/505 [==============================] - 232s 460ms/step - loss: 0.3881 - acc: 0.9934 - f1_m: 0.9889 - val_loss: 0.3020 - val_acc: 0.9271 - val_f1_m: 0.9335\n",
      "\n",
      "Epoch 00051: val_f1_m did not improve from 0.93760\n",
      "Epoch 52/150\n",
      "505/505 [==============================] - 232s 459ms/step - loss: 0.3833 - acc: 0.9931 - f1_m: 0.9883 - val_loss: 0.2717 - val_acc: 0.9322 - val_f1_m: 0.9250\n",
      "\n",
      "Epoch 00052: val_f1_m did not improve from 0.93760\n",
      "Epoch 53/150\n",
      "505/505 [==============================] - 232s 459ms/step - loss: 0.3851 - acc: 0.9918 - f1_m: 0.9870 - val_loss: 0.3290 - val_acc: 0.9271 - val_f1_m: 0.9232\n",
      "\n",
      "Epoch 00053: val_f1_m did not improve from 0.93760\n",
      "Epoch 54/150\n",
      "505/505 [==============================] - 232s 459ms/step - loss: 0.3864 - acc: 0.9933 - f1_m: 0.9886 - val_loss: 0.3315 - val_acc: 0.9104 - val_f1_m: 0.9144\n",
      "\n",
      "Epoch 00054: val_f1_m did not improve from 0.93760\n",
      "Epoch 55/150\n",
      "505/505 [==============================] - 232s 459ms/step - loss: 0.3850 - acc: 0.9927 - f1_m: 0.9889 - val_loss: 0.2620 - val_acc: 0.9437 - val_f1_m: 0.9390\n",
      "\n",
      "Epoch 00055: val_f1_m improved from 0.93760 to 0.93899, saving model to ../model/emsemble_final_EfficientNetB3_fold_5.hdf5\n",
      "Epoch 56/150\n",
      "505/505 [==============================] - 232s 460ms/step - loss: 0.3836 - acc: 0.9929 - f1_m: 0.9902 - val_loss: 0.3265 - val_acc: 0.9301 - val_f1_m: 0.9277\n",
      "\n",
      "Epoch 00056: val_f1_m did not improve from 0.93899\n",
      "Epoch 57/150\n",
      "505/505 [==============================] - 232s 459ms/step - loss: 0.3805 - acc: 0.9925 - f1_m: 0.9877 - val_loss: 0.3587 - val_acc: 0.9146 - val_f1_m: 0.9149\n",
      "\n",
      "Epoch 00057: val_f1_m did not improve from 0.93899\n",
      "Epoch 58/150\n",
      "256/505 [==============>...............] - ETA: 1:52 - loss: 0.3858 - acc: 0.9927 - f1_m: 0.9890"
     ]
    }
   ],
   "source": [
    "rs_models, rs_histories = train(model_type = model_type,\n",
    "                                skf_seed=5,\n",
    "                                n_splits=n_splits,\n",
    "                                opt=get_opt('rmsprop', lr=lr))\n",
    "mymodels[model_type] = rs_models\n",
    "myhistories[model_type] = rs_histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fold1 - step 37\n",
    "#loss: 0.4194 - acc: 0.9879 - f1_m: 0.9828 - val_loss: 0.2951 - val_acc: 0.9394 - val_f1_m: 0.9391\n",
    "#fold2 - step 66\n",
    "#loss: 0.3689 - acc: 0.9959 - f1_m: 0.9917 - val_loss: 0.2223 - val_acc: 0.9609 - val_f1_m: 0.9562\n",
    "#fold3 - step 50\n",
    "#loss: 0.4076 - acc: 0.9904 - f1_m: 0.9840 - val_loss: 0.2574 - val_acc: 0.9395 - val_f1_m: 0.9458\n",
    "#fold4 - step 57\n",
    "#loss: 0.3693 - acc: 0.9950 - f1_m: 0.9916 - val_loss: 0.2222 - val_acc: 0.9556 - val_f1_m: 0.9459\n",
    "#fold5 - step 55\n",
    "#loss: 0.3850 - acc: 0.9927 - f1_m: 0.9889 - val_loss: 0.2620 - val_acc: 0.9437 - val_f1_m: 0.9390"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/function.py:987: calling Graph.create_op (from tensorflow.python.framework.ops) with compute_shapes is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Shapes are always computed; don't use the compute_shapes as it has no effect.\n"
     ]
    }
   ],
   "source": [
    "model_type = 'EfficientNetB3'\n",
    "model = get_model(model_type=model_type,\n",
    "                  image_size=299,\n",
    "                  opt=get_opt('sgd', lr=lr),\n",
    "                  lr=lr,\n",
    "                  activation=params[model_type]['activation'],\n",
    "                  dense=params[model_type]['dense'],\n",
    "                  drop=params[model_type]['drop'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen, valid_datagen, test_datagen = get_datagen(ismixup=True, israndombox=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9990 validated image filenames belonging to 196 classes.\n",
      "Found 9990 validated image filenames belonging to 196 classes.\n",
      "Found 9990 validated image filenames belonging to 196 classes.\n",
      "Found 6150 validated image filenames.\n",
      "193/193 [==============================] - 42s 216ms/step\n",
      "193/193 [==============================] - 40s 206ms/step\n",
      "193/193 [==============================] - 40s 206ms/step\n",
      "193/193 [==============================] - 41s 210ms/step\n",
      "193/193 [==============================] - 40s 210ms/step\n",
      "193/193 [==============================] - 40s 209ms/step\n",
      "193/193 [==============================] - 40s 207ms/step\n",
      "193/193 [==============================] - 39s 204ms/step\n",
      "193/193 [==============================] - 40s 208ms/step\n",
      "193/193 [==============================] - 40s 206ms/step\n",
      "Found 9990 validated image filenames belonging to 196 classes.\n",
      "Found 9990 validated image filenames belonging to 196 classes.\n",
      "Found 9990 validated image filenames belonging to 196 classes.\n",
      "Found 6150 validated image filenames.\n",
      "193/193 [==============================] - 38s 199ms/step\n",
      "193/193 [==============================] - 38s 199ms/step\n",
      "193/193 [==============================] - 38s 198ms/step\n",
      "193/193 [==============================] - 38s 198ms/step\n",
      "193/193 [==============================] - 39s 201ms/step\n",
      "193/193 [==============================] - 39s 204ms/step\n",
      "193/193 [==============================] - 38s 199ms/step\n",
      "193/193 [==============================] - 40s 209ms/step\n",
      "193/193 [==============================] - 40s 208ms/step\n",
      "193/193 [==============================] - 39s 204ms/step\n",
      "Found 9990 validated image filenames belonging to 196 classes.\n",
      "Found 9990 validated image filenames belonging to 196 classes.\n",
      "Found 9990 validated image filenames belonging to 196 classes.\n",
      "Found 6150 validated image filenames.\n",
      "193/193 [==============================] - 40s 209ms/step\n",
      "193/193 [==============================] - 39s 200ms/step\n",
      "193/193 [==============================] - 38s 197ms/step\n",
      "193/193 [==============================] - 38s 198ms/step\n",
      "193/193 [==============================] - 38s 199ms/step\n",
      "193/193 [==============================] - 39s 200ms/step\n",
      "193/193 [==============================] - 38s 197ms/step\n",
      "193/193 [==============================] - 39s 203ms/step\n",
      "193/193 [==============================] - 40s 209ms/step\n",
      "193/193 [==============================] - 40s 208ms/step\n",
      "Found 9990 validated image filenames belonging to 196 classes.\n",
      "Found 9990 validated image filenames belonging to 196 classes.\n",
      "Found 9990 validated image filenames belonging to 196 classes.\n",
      "Found 6150 validated image filenames.\n",
      "193/193 [==============================] - 39s 200ms/step\n",
      "193/193 [==============================] - 39s 201ms/step\n",
      "193/193 [==============================] - 38s 198ms/step\n",
      "193/193 [==============================] - 38s 195ms/step\n",
      "193/193 [==============================] - 38s 195ms/step\n",
      "193/193 [==============================] - 39s 203ms/step\n",
      "193/193 [==============================] - 39s 205ms/step\n",
      "193/193 [==============================] - 39s 204ms/step\n",
      "193/193 [==============================] - 39s 204ms/step\n",
      "193/193 [==============================] - 39s 203ms/step\n",
      "Found 9990 validated image filenames belonging to 196 classes.\n",
      "Found 9990 validated image filenames belonging to 196 classes.\n",
      "Found 9990 validated image filenames belonging to 196 classes.\n",
      "Found 6150 validated image filenames.\n",
      "193/193 [==============================] - 38s 196ms/step\n",
      "193/193 [==============================] - 38s 196ms/step\n",
      "193/193 [==============================] - 38s 196ms/step\n",
      "193/193 [==============================] - 38s 196ms/step\n",
      "193/193 [==============================] - 38s 196ms/step\n",
      "193/193 [==============================] - 38s 196ms/step\n",
      "193/193 [==============================] - 40s 208ms/step\n",
      "193/193 [==============================] - 39s 203ms/step\n",
      "193/193 [==============================] - 40s 208ms/step\n",
      "193/193 [==============================] - 41s 212ms/step\n"
     ]
    }
   ],
   "source": [
    "test_batch_size = 32\n",
    "y_pred = []\n",
    "for fold_step in range(n_splits):\n",
    "    predictions = []\n",
    "\n",
    "\n",
    "    model_path =model_dir +  \"{}_{}_fold_{}.{}\".format(model_title, model_type, fold_step+1, 'hdf5')\n",
    "    model.load_weights(model_path)\n",
    "\n",
    "    train_generator, _, test_generator = get_generator(train_df=df_train,\n",
    "                                                        val_df=df_train,\n",
    "                                                        test_df=df_test,\n",
    "                                                        train_datagen = train_datagen,\n",
    "                                                        valid_datagen = valid_datagen,\n",
    "                                                        test_datagen= test_datagen,\n",
    "                                                        image_size=299,\n",
    "                                                        test_batch_size=test_batch_size\n",
    "                                                        )\n",
    "    test_generator.reset()\n",
    "\n",
    "    for i in range(10):\n",
    "        prediction = model.predict_generator(\n",
    "            generator=test_generator,\n",
    "            steps = get_steps(df_test.shape[0], test_batch_size),\n",
    "            verbose=1\n",
    "        )\n",
    "        predictions.append(prediction)\t\n",
    "    y_pred.append(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fold1 - step 37\n",
    "#loss: 0.4194 - acc: 0.9879 - f1_m: 0.9828 - val_loss: 0.2951 - val_acc: 0.9394 - val_f1_m: 0.9391\n",
    "#fold2 - step 66\n",
    "#loss: 0.3689 - acc: 0.9959 - f1_m: 0.9917 - val_loss: 0.2223 - val_acc: 0.9609 - val_f1_m: 0.9562\n",
    "#fold3 - step 50\n",
    "#loss: 0.4076 - acc: 0.9904 - f1_m: 0.9840 - val_loss: 0.2574 - val_acc: 0.9395 - val_f1_m: 0.9458\n",
    "#fold4 - step 57\n",
    "#loss: 0.3693 - acc: 0.9950 - f1_m: 0.9916 - val_loss: 0.2222 - val_acc: 0.9556 - val_f1_m: 0.9459\n",
    "#fold5 - step 55\n",
    "#loss: 0.3850 - acc: 0.9927 - f1_m: 0.9889 - val_loss: 0.2620 - val_acc: 0.9437 - val_f1_m: 0.9390"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9990 validated image filenames belonging to 196 classes.\n",
      "Found 9990 validated image filenames belonging to 196 classes.\n",
      "Found 9990 validated image filenames belonging to 196 classes.\n",
      "Found 6150 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "train_generator, _, test_generator = get_generator(train_df=df_train,\n",
    "                                                        val_df=df_train,\n",
    "                                                        test_df=df_test,\n",
    "                                                        train_datagen = train_datagen,\n",
    "                                                        valid_datagen = valid_datagen,\n",
    "                                                        test_datagen= test_datagen,\n",
    "                                                        image_size=299,\n",
    "                                                        test_batch_size=32\n",
    "                                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 10, 6150, 196)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.array(y_pred)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('y_pred_mixup_fold5_tta10', y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.load('y_pred_mixup_fold5_tta10.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_file</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_00001.jpg</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_00002.jpg</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_00003.jpg</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_00004.jpg</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_00005.jpg</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         img_file class\n",
       "0  test_00001.jpg   124\n",
       "1  test_00002.jpg    98\n",
       "2  test_00003.jpg   157\n",
       "3  test_00004.jpg    94\n",
       "4  test_00005.jpg    17"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold_all = y_pred.reshape([-1, 6150, 196])\n",
    "fold_all = np.max(fold_all, axis=0)\n",
    "predicted_class_indices=np.argmax(fold_all, axis=1)\n",
    "labels = (train_generator.generator1.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "predictions_labels = [labels[k] for k in predicted_class_indices]\n",
    "submission = pd.read_csv(os.path.join(DATA_PATH, 'sample_submission.csv'))\n",
    "submission[\"class\"] = predictions_labels\n",
    "submission.to_csv(\"submission_{}_fold5_mixup_ensemble_max_tta10_fold_sum.csv\".format(model_type), index=False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_file</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_00001.jpg</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_00002.jpg</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_00003.jpg</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_00004.jpg</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_00005.jpg</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         img_file class\n",
       "0  test_00001.jpg   124\n",
       "1  test_00002.jpg    98\n",
       "2  test_00003.jpg   157\n",
       "3  test_00004.jpg    94\n",
       "4  test_00005.jpg    18"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold_2 = y_pred[3,:,:,:]\n",
    "fold_2 = fold_2.reshape([-1, 6150, 196])\n",
    "fold_2 = np.max(fold_2, axis=0)\n",
    "predicted_class_indices=np.argmax(fold_2, axis=1)\n",
    "labels = (train_generator.generator1.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "predictions_labels = [labels[k] for k in predicted_class_indices]\n",
    "submission = pd.read_csv(os.path.join(DATA_PATH, 'sample_submission.csv'))\n",
    "submission[\"class\"] = predictions_labels\n",
    "submission.to_csv(\"submission_{}_fold5_mixup_ensemble_max_tta10_fold_2.csv\".format(model_type), index=False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
